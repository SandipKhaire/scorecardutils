{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_credit_risk_data(n_samples=50000, default_rate=0.10):\n",
    "    \"\"\"\n",
    "    Generate synthetic credit risk data with specified characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of data points to generate\n",
    "    - default_rate: Proportion of default cases (target=1)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with features and target variable\n",
    "    \"\"\"\n",
    "    # Calculate number of defaults\n",
    "    n_defaults = int(n_samples * default_rate)\n",
    "    n_non_defaults = n_samples - n_defaults\n",
    "    \n",
    "    # Generate base data with 10 informative features\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=10,  # We'll expand to 50 later\n",
    "        n_informative=8,  # Truly informative\n",
    "        n_redundant=2,   # Correlated features\n",
    "        n_classes=2,\n",
    "        weights=[1-default_rate, default_rate],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    feature_names = [f\"feature_{i+1}\" for i in range(X.shape[1])]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    \n",
    "    # Rename the first 10 features to meaningful names\n",
    "    # Highly predictive numeric variables (6)\n",
    "    df.rename(columns={\n",
    "        'feature_1': 'income',\n",
    "        'feature_2': 'debt_to_income_ratio',\n",
    "        'feature_3': 'credit_score',\n",
    "        'feature_4': 'loan_amount', \n",
    "        'feature_5': 'interest_rate',\n",
    "        'feature_6': 'age',\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Create a highly correlated feature with income\n",
    "    df['total_assets'] = df['income'] * np.random.normal(3, 0.2, n_samples) + np.random.normal(0, 0.5, n_samples)\n",
    "    \n",
    "    # Transform features to more realistic ranges\n",
    "    df['income'] = (df['income'] * 20000 + 50000).clip(10000, 250000)\n",
    "    df['debt_to_income_ratio'] = (df['debt_to_income_ratio'] + 3) / 6  # Scale to 0-1 range\n",
    "    df['debt_to_income_ratio'] = (df['debt_to_income_ratio'] * 0.6 + 0.1).clip(0.05, 0.8)  \n",
    "    df['credit_score'] = (df['credit_score'] * 150 + 650).clip(300, 850).astype(int)\n",
    "    df['loan_amount'] = (df['loan_amount'] * 50000 + 100000).clip(5000, 500000)\n",
    "    df['interest_rate'] = (df['interest_rate'] + 3) / 6 * 15 + 2  # Interest rate between 2% and 17%\n",
    "    df['age'] = (df['age'] * 20 + 40).clip(18, 85).astype(int)\n",
    "    \n",
    "    # Highly predictive categorical variables (4)\n",
    "    # Feature 7: employment_status (categorical)\n",
    "    employment_statuses = ['Employed', 'Self-Employed', 'Unemployed', 'Retired']\n",
    "    # Make employment more predictive - separate probability arrays for default vs non-default\n",
    "    employed_prob_default = [0.3, 0.2, 0.4, 0.1]\n",
    "    employed_prob_nondefault = [0.7, 0.2, 0.05, 0.05]\n",
    "    \n",
    "    # Create array to store employment status\n",
    "    employment_status = []\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:\n",
    "            employment_status.append(np.random.choice(employment_statuses, p=employed_prob_default))\n",
    "        else:\n",
    "            employment_status.append(np.random.choice(employment_statuses, p=employed_prob_nondefault))\n",
    "    \n",
    "    df['employment_status'] = employment_status\n",
    "    \n",
    "    # Feature 8: education_level (categorical)\n",
    "    education_levels = ['High School', 'Bachelor', 'Master', 'PhD', 'Other']\n",
    "    # Make education more predictive - separate probability arrays\n",
    "    education_prob_default = [0.5, 0.3, 0.1, 0.05, 0.05]\n",
    "    education_prob_nondefault = [0.2, 0.4, 0.3, 0.08, 0.02]\n",
    "    \n",
    "    # Create array to store education level\n",
    "    education_level = []\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:\n",
    "            education_level.append(np.random.choice(education_levels, p=education_prob_default))\n",
    "        else:\n",
    "            education_level.append(np.random.choice(education_levels, p=education_prob_nondefault))\n",
    "    \n",
    "    df['education_level'] = education_level\n",
    "    \n",
    "    # Feature 9: loan_purpose (categorical)\n",
    "    loan_purposes = ['Home', 'Auto', 'Education', 'Personal', 'Business', 'Debt Consolidation']\n",
    "    # Make loan_purpose more predictive\n",
    "    purpose_prob_default = [0.1, 0.15, 0.2, 0.2, 0.25, 0.1]\n",
    "    purpose_prob_nondefault = [0.3, 0.2, 0.1, 0.1, 0.1, 0.2]\n",
    "    \n",
    "    # Create array to store loan purpose\n",
    "    loan_purpose = []\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:\n",
    "            loan_purpose.append(np.random.choice(loan_purposes, p=purpose_prob_default))\n",
    "        else:\n",
    "            loan_purpose.append(np.random.choice(loan_purposes, p=purpose_prob_nondefault))\n",
    "    \n",
    "    df['loan_purpose'] = loan_purpose\n",
    "    \n",
    "    # Feature 10: has_previous_defaults (categorical but binary)\n",
    "    # Make previous defaults highly predictive\n",
    "    has_previous_defaults = []\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:\n",
    "            # 70% of defaulters have previous defaults\n",
    "            has_previous_defaults.append(np.random.choice([1, 0], p=[0.7, 0.3]))\n",
    "        else:\n",
    "            # 10% of non-defaulters have previous defaults\n",
    "            has_previous_defaults.append(np.random.choice([1, 0], p=[0.1, 0.9]))\n",
    "    \n",
    "    df['has_previous_defaults'] = [('Yes' if val == 1 else 'No') for val in has_previous_defaults]\n",
    "    \n",
    "    # Create a correlated categorical variable with employment_status\n",
    "    # payment_history (correlated with employment_status)\n",
    "    payment_history_map = {\n",
    "        'Employed': np.array(['Excellent', 'Good', 'Fair', 'Poor']),\n",
    "        'Self-Employed': np.array(['Good', 'Fair', 'Fair', 'Poor']),\n",
    "        'Unemployed': np.array(['Fair', 'Poor', 'Poor', 'Poor']), \n",
    "        'Retired': np.array(['Excellent', 'Good', 'Fair', 'Poor'])\n",
    "    }\n",
    "    \n",
    "    # Create probabilities for payment_history based on employment status\n",
    "    payment_probs = {\n",
    "        'Employed': [0.5, 0.3, 0.15, 0.05],\n",
    "        'Self-Employed': [0.3, 0.4, 0.2, 0.1],\n",
    "        'Unemployed': [0.1, 0.2, 0.3, 0.4],\n",
    "        'Retired': [0.4, 0.3, 0.2, 0.1]\n",
    "    }\n",
    "    \n",
    "    payment_history = []\n",
    "    for status in df['employment_status']:\n",
    "        payment_history.append(np.random.choice(payment_history_map[status], p=payment_probs[status]))\n",
    "    \n",
    "    df['payment_history'] = payment_history\n",
    "    \n",
    "    # Add remaining 40 less predictive features (mix of numeric and categorical)\n",
    "    # Numeric features (30)\n",
    "    for i in range(1, 31):\n",
    "        # Generate less predictive numeric features\n",
    "        feature_name = f'numeric_feature_{i}'\n",
    "        if i <= 5:  # First 5 slightly more predictive than the rest\n",
    "            feature_values = np.random.normal(0, 1, n_samples) + y * np.random.uniform(0.1, 0.3)\n",
    "        else:  # Remaining 25 features are mostly noise\n",
    "            feature_values = np.random.normal(0, 1, n_samples) + y * np.random.uniform(0, 0.1)\n",
    "            \n",
    "        # Apply different transformations to make features diverse\n",
    "        if i % 4 == 0:\n",
    "            # Exponential-like features (e.g., transaction amounts)\n",
    "            feature_values = np.exp(feature_values * 0.5) * 100\n",
    "        elif i % 4 == 1:\n",
    "            # Percentage-like features (e.g., utilization rates)\n",
    "            feature_values = stats.norm.cdf(feature_values) * 100\n",
    "        elif i % 4 == 2:\n",
    "            # Count-like features (e.g., number of inquiries)\n",
    "            feature_values = np.abs(feature_values * 5).astype(int)\n",
    "        # else leave as standard normal\n",
    "            \n",
    "        df[feature_name] = feature_values\n",
    "    \n",
    "    # Categorical features (10)\n",
    "    categorical_vars = [\n",
    "        ('marital_status', ['Single', 'Married', 'Divorced', 'Widowed']),\n",
    "        ('housing_status', ['Own', 'Mortgage', 'Rent', 'Other']),\n",
    "        ('job_industry', ['Technology', 'Healthcare', 'Finance', 'Education', 'Manufacturing', 'Retail', 'Other']),\n",
    "        ('state', ['CA', 'NY', 'TX', 'FL', 'IL', 'PA', 'OH', 'GA', 'Other']),\n",
    "        ('credit_card_type', ['Visa', 'Mastercard', 'Amex', 'Discover', 'None']),\n",
    "        ('num_dependents', [0, 1, 2, 3, 4, '5+']),\n",
    "        ('months_at_current_job', ['<6', '6-12', '1-3 years', '3-5 years', '5+ years']),\n",
    "        ('has_cosigner', ['Yes', 'No']),\n",
    "        ('account_type', ['Checking', 'Savings', 'Both', 'None']),\n",
    "        ('application_channel', ['Online', 'In-person', 'Phone', 'Mail'])\n",
    "    ]\n",
    "    \n",
    "    for i, (feature_name, categories) in enumerate(categorical_vars):\n",
    "        # For first 3 categorical variables, make them slightly predictive\n",
    "        if i < 3:\n",
    "            # Different probability distributions based on target\n",
    "            p_default = np.random.dirichlet(np.ones(len(categories)) * 2)\n",
    "            p_non_default = np.random.dirichlet(np.ones(len(categories)) * 2)\n",
    "            \n",
    "            # Ensure some difference between distributions\n",
    "            max_idx = np.argmax(p_default)\n",
    "            p_default[max_idx] += 0.1\n",
    "            p_default = p_default / sum(p_default)\n",
    "            \n",
    "            # Choose categories based on target\n",
    "            cat_values = []\n",
    "            for target_val in y:\n",
    "                if target_val == 1:\n",
    "                    cat_values.append(np.random.choice(categories, p=p_default))\n",
    "                else:\n",
    "                    cat_values.append(np.random.choice(categories, p=p_non_default))\n",
    "        else:\n",
    "            # For the rest, almost no predictive power\n",
    "            cat_values = np.random.choice(categories, size=n_samples)\n",
    "            \n",
    "        df[feature_name] = cat_values\n",
    "    \n",
    "    # Add target variable\n",
    "    df['default'] = y\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "credit_risk_df = generate_credit_risk_data(n_samples=50000, default_rate=0.10)\n",
    "\n",
    "# Verify the default rate\n",
    "default_rate = credit_risk_df['default'].mean()\n",
    "print(f\"Default rate in the dataset: {default_rate:.4f}\")\n",
    "\n",
    "# Check correlation between income and total_assets (should be highly correlated)\n",
    "income_assets_corr = credit_risk_df['income'].corr(credit_risk_df['total_assets'])\n",
    "print(f\"Correlation between income and total_assets: {income_assets_corr:.4f}\")\n",
    "\n",
    "# Check correlation between employment_status and payment_history (categorical correlation)\n",
    "crosstab = pd.crosstab(credit_risk_df['employment_status'], credit_risk_df['payment_history'])\n",
    "print(\"\\nCrosstab of employment_status and payment_history:\")\n",
    "print(crosstab)\n",
    "\n",
    "# Show information about the dataset\n",
    "print(\"\\nDataset information:\")\n",
    "print(f\"Total rows: {len(credit_risk_df)}\")\n",
    "print(f\"Total columns: {len(credit_risk_df.columns)}\")\n",
    "print(f\"Default cases: {credit_risk_df['default'].sum()}\")\n",
    "print(f\"Non-default cases: {len(credit_risk_df) - credit_risk_df['default'].sum()}\")\n",
    "\n",
    "# Show a sample of the data\n",
    "print(\"\\nSample of the generated dataset:\")\n",
    "print(credit_risk_df.head())\n",
    "\n",
    "# Feature importance analysis\n",
    "# We'll use correlation for numeric features and chi-square for categorical\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Analyze numeric features\n",
    "numeric_features = credit_risk_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_features = [col for col in numeric_features if col != 'default']\n",
    "\n",
    "print(\"\\nCorrelation of numeric features with default:\")\n",
    "correlations = {}\n",
    "for col in numeric_features:\n",
    "    corr = credit_risk_df[col].corr(credit_risk_df['default'])\n",
    "    correlations[col] = abs(corr)\n",
    "\n",
    "sorted_numeric = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "for feature, corr in sorted_numeric[:15]:  # Show top 15\n",
    "    print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Analyze categorical features\n",
    "categorical_features = credit_risk_df.select_dtypes(include=['object']).columns\n",
    "print(\"\\nChi-square statistics for categorical features:\")\n",
    "chi2_values = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    contingency = pd.crosstab(credit_risk_df[col], credit_risk_df['default'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    chi2_values[col] = (chi2, p)\n",
    "\n",
    "sorted_categorical = sorted(chi2_values.items(), key=lambda x: x[1][0], reverse=True)\n",
    "for feature, (chi2, p) in sorted_categorical:\n",
    "    print(f\"{feature}: Chi2={chi2:.2f}, p-value={p:.6f}\")\n",
    "\n",
    "# Save to CSV\n",
    "credit_risk_df.to_csv('../data/credit_risk_dataset.csv', index=False)\n",
    "print(\"\\nDataset saved to 'credit_risk_dataset.csv'\")\n",
    "\n",
    "# Summary of highly predictive features\n",
    "print(\"\\nHighly predictive features:\")\n",
    "print(\"Numeric: income, debt_to_income_ratio, credit_score, loan_amount, interest_rate, age\")\n",
    "print(\"Correlated numeric pair: income and total_assets\")\n",
    "print(\"Categorical: employment_status, education_level, loan_purpose, has_previous_defaults\")\n",
    "print(\"Correlated categorical pair: employment_status and payment_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Generate a synthetic dataset with 10 predictive features\n",
    "X, y = make_classification(\n",
    "    n_samples=50000,\n",
    "    n_features=10,  # 10 predictive features\n",
    "    n_informative=8,  # 8 truly informative features\n",
    "    n_redundant=2,    # 2 redundant features from the informative ones\n",
    "    n_classes=2,      # Binary target\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create a DataFrame with the features and target\n",
    "feature_names = [f'feature_{i+1}' for i in range(10)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Add 5 redundant features\n",
    "# Redundant feature 1: copy of feature_1 with noise\n",
    "df['redundant_1'] = df['feature_1'] + np.random.normal(0, 0.1, size=50000)\n",
    "\n",
    "# Redundant feature 2: linear combination of feature_2 and feature_3\n",
    "df['redundant_2'] = 0.7 * df['feature_2'] + 0.3 * df['feature_3'] + np.random.normal(0, 0.05, size=50000)\n",
    "\n",
    "# Redundant feature 3: copy of feature_4 with different scale\n",
    "df['redundant_3'] = 2.5 * df['feature_4'] + np.random.normal(0, 0.1, size=50000)\n",
    "\n",
    "# Redundant feature 4: transformation of feature_5\n",
    "df['redundant_4'] = np.log(np.abs(df['feature_5']) + 1) + np.random.normal(0, 0.1, size=50000)\n",
    "\n",
    "# Redundant feature 5: feature_6 with offset\n",
    "df['redundant_5'] = df['feature_6'] + 1.5 + np.random.normal(0, 0.08, size=50000)\n",
    "\n",
    "# Convert some numerical features to categorical\n",
    "# Convert feature_3 to categorical (low, medium, high)\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "df['feature_3_cat'] = discretizer.fit_transform(df[['feature_3']])\n",
    "df['feature_3_cat'] = df['feature_3_cat'].map({0.0: 'low', 1.0: 'medium', 2.0: 'high'})\n",
    "df = df.drop('feature_3', axis=1)\n",
    "\n",
    "# Convert feature_7 to binary categorical\n",
    "df['feature_7_cat'] = np.where(df['feature_7'] > 0, 'yes', 'no')\n",
    "df = df.drop('feature_7', axis=1)\n",
    "\n",
    "# Convert feature_10 to multi-level categorical\n",
    "discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')\n",
    "df['feature_10_cat'] = discretizer.fit_transform(df[['feature_10']])\n",
    "df['feature_10_cat'] = df['feature_10_cat'].map({0.0: 'very_low', 1.0: 'low', 2.0: 'high', 3.0: 'very_high'})\n",
    "df = df.drop('feature_10', axis=1)\n",
    "\n",
    "# Convert redundant_5 to categorical\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n",
    "df['redundant_5_cat'] = discretizer.fit_transform(df[['redundant_5']])\n",
    "df['redundant_5_cat'] = df['redundant_5_cat'].map({0.0: 'group_A', 1.0: 'group_B', 2.0: 'group_C'})\n",
    "df = df.drop('redundant_5', axis=1)\n",
    "\n",
    "# Introduce missing values\n",
    "# 1. Missing values in a categorical variable (feature_3_cat)\n",
    "mask_cat = np.random.choice([True, False], size=df.shape[0], p=[0.05, 0.95])  # 5% missing values\n",
    "df.loc[mask_cat, 'feature_3_cat'] = np.nan\n",
    "\n",
    "# 2. Missing values in numeric variable feature_1\n",
    "mask_num1 = np.random.choice([True, False], size=df.shape[0], p=[0.08, 0.92])  # 8% missing values\n",
    "df.loc[mask_num1, 'feature_1'] = np.nan\n",
    "\n",
    "# 3. Missing values in numeric variable feature_5\n",
    "mask_num2 = np.random.choice([True, False], size=df.shape[0], p=[0.1, 0.9])  # 10% missing values\n",
    "df.loc[mask_num2, 'feature_5'] = np.nan\n",
    "\n",
    "# Rename columns for better clarity\n",
    "df = df.rename(columns={\n",
    "    'feature_1': 'age',\n",
    "    'feature_2': 'income',\n",
    "    'feature_4': 'experience_years',\n",
    "    'feature_5': 'credit_score',\n",
    "    'feature_6': 'avg_spend',\n",
    "    'feature_8': 'loyalty_score',\n",
    "    'feature_9': 'satisfaction_rating',\n",
    "    'feature_3_cat': 'education_level',\n",
    "    'feature_7_cat': 'is_homeowner',\n",
    "    'feature_10_cat': 'customer_segment',\n",
    "    'redundant_1': 'demographic_index',\n",
    "    'redundant_2': 'financial_status',\n",
    "    'redundant_3': 'work_experience',\n",
    "    'redundant_4': 'credit_index',\n",
    "    'redundant_5_cat': 'customer_group'\n",
    "})\n",
    "\n",
    "# Reorder columns with target at the end\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('target')\n",
    "cols.append('target')\n",
    "df = df[cols]\n",
    "\n",
    "# Display information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFeature types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nSample of the dataset (first 10 rows):\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('../data/synthetic_binary_classification_data.csv', index=False)\n",
    "print(\"\\nDataset saved to 'synthetic_binary_classification_data.csv'\")\n",
    "\n",
    "# Additional information about the dataset for interpretation\n",
    "print(\"\\nDataset Information:\")\n",
    "print(\"- Binary target with approximately 50% distribution\")\n",
    "print(\"- 15 features total: 10 predictive and 5 redundant\")\n",
    "print(\"- 11 numerical features and 4 categorical features\")\n",
    "print(\"- Missing values in 3 columns: education_level (cat), age (num), credit_score (num)\")\n",
    "print(\"\\nRedundant features:\")\n",
    "print(\"- demographic_index: redundant with age\")\n",
    "print(\"- financial_status: redundant with income and experience_years\")\n",
    "print(\"- work_experience: redundant with experience_years\")\n",
    "print(\"- credit_index: redundant with credit_score\")\n",
    "print(\"- customer_group: redundant with avg_spend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
