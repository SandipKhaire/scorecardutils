{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample continuous variables\n",
    "np.random.seed(42)  # For reproducibility\n",
    "x = np.random.normal(loc=50, scale=10, size=1000)\n",
    "y = x * 0.5 + np.random.normal(loc=0, scale=2, size=1000)  # Add some correlation\n",
    "y = 1 * y\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'X': x, 'Y': y})\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = df.corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/sandip/Documents/PythonProjects/PythonApps/example_uv/src')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "x = np.random.normal(loc=50, scale=10, size=100)\n",
    "y = x * 2 + np.random.normal(loc=0, scale=1, size=100)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'X': -3* x, 'Y': y})\n",
    "\n",
    "# Calculate the correlation (optional, just to verify)\n",
    "correlation = df.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation)\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['X'], df['Y'], color='blue', alpha=0.7)\n",
    "plt.title(\"Scatter Plot of X vs Y\", fontsize=14)\n",
    "plt.xlabel(\"X\", fontsize=12)\n",
    "plt.ylabel(\"Y\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate correlated X1 and X2\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "X1 = np.random.normal(0, 1, n)\n",
    "X2 = 0.8 * X1 + np.sqrt(1 - 0.8**2) * np.random.normal(0, 1, n)  # Correlation â‰ˆ 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Create target variable Y from a logistic model using both X1 and X2\n",
    "logit = 1.5 * X1 + 1.5 * X2\n",
    "prob = 1 / (1 + np.exp(-logit))\n",
    "Y = np.random.binomial(1, prob)\n",
    "\n",
    "# 3. Correlation between X1 and X2\n",
    "corr_X1_X2, _ = pearsonr(X1, X2)\n",
    "print(\"Correlation between X1 and X2:\", round(corr_X1_X2, 3))\n",
    "\n",
    "# 4. Logistic regression Y ~ X1 (no intercept)\n",
    "model_x1 = LogisticRegression(fit_intercept=False, solver='lbfgs')\n",
    "model_x1.fit(X1.reshape(-1, 1), Y)\n",
    "pred1 = model_x1.predict_proba(X1.reshape(-1, 1))[:, 1]\n",
    "\n",
    "# 5. Logistic regression Y ~ X2 (no intercept)\n",
    "model_x2 = LogisticRegression(fit_intercept=False, solver='lbfgs')\n",
    "model_x2.fit(X2.reshape(-1, 1), Y)\n",
    "pred2 = model_x2.predict_proba(X2.reshape(-1, 1))[:, 1]\n",
    "\n",
    "# 6. Correlation between predicted probabilities\n",
    "corr_pred1_pred2, _ = pearsonr(pred1, pred2)\n",
    "print(\"Correlation between predicted Y1 and Y2:\", round(corr_pred1_pred2, 3))\n",
    "\n",
    "# 7. Compare\n",
    "print(f\"\\nOriginal X1-X2 Corr: {corr_X1_X2:.3f}\")\n",
    "print(f\"Predicted Y1-Y2 Corr: {corr_pred1_pred2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "\n",
    "# Generate correlated predictors X1 and X2 with various correlation values\n",
    "correlations = np.arange(0, 1.1, 0.1)\n",
    "results = []\n",
    "\n",
    "for corr in correlations:\n",
    "    # Create covariance matrix for correlated variables\n",
    "    cov_matrix = np.array([[1, corr], [corr, 1]])\n",
    "    \n",
    "    # Generate correlated X1 and X2\n",
    "    X = np.random.multivariate_normal(mean=[0, 0], cov=cov_matrix, size=n)\n",
    "    X1 = X[:, 0]\n",
    "    X2 = X[:, 1]\n",
    "    \n",
    "    # Calculate direct correlation between X1 and X2\n",
    "    direct_corr, _ = pearsonr(X1, X2)\n",
    "    \n",
    "    # Generate binary target Y (using a linear combination of X1 and X2)\n",
    "    # We'll use both variables to generate Y to ensure both have some predictive power\n",
    "    z = 0.7*X1 + 0.7*X2 + np.random.normal(0, 1, n)\n",
    "    prob = 1 / (1 + np.exp(-z))  # logistic function\n",
    "    Y = (prob > 0.5).astype(int)\n",
    "    \n",
    "    # Fit logistic regression on X1 (without intercept)\n",
    "    model1 = LogisticRegression(fit_intercept=False, max_iter=1000)\n",
    "    model1.fit(X1.reshape(-1, 1), Y)\n",
    "    \n",
    "    # Fit logistic regression on X2 (without intercept)\n",
    "    model2 = LogisticRegression(fit_intercept=False, max_iter=1000)\n",
    "    model2.fit(X2.reshape(-1, 1), Y)\n",
    "    \n",
    "    # Get predictions (probabilities)\n",
    "    pred1 = model1.predict_proba(X1.reshape(-1, 1))[:, 1]\n",
    "    pred2 = model2.predict_proba(X2.reshape(-1, 1))[:, 1]\n",
    "    \n",
    "    # Calculate correlation between predictions\n",
    "    pred_corr, _ = pearsonr(pred1, pred2)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Input_Correlation': direct_corr,\n",
    "        'Prediction_Correlation': pred_corr\n",
    "    })\n",
    "\n",
    "# Create dataframe from results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Plot the relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Input_Correlation', y='Prediction_Correlation', data=df_results)\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='y=x line')\n",
    "plt.title('Correlation: Input Variables vs. Logistic Regression Predictions')\n",
    "plt.xlabel('Correlation between X1 and X2')\n",
    "plt.ylabel('Correlation between predictions from X1 and X2 models')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Print the results table\n",
    "print(\"Results Table:\")\n",
    "print(df_results.round(4))\n",
    "\n",
    "# Let's also visualize a specific case with scatterplots\n",
    "# Choose a moderate correlation value (0.5)\n",
    "corr = 0.5\n",
    "cov_matrix = np.array([[1, corr], [corr, 1]])\n",
    "X = np.random.multivariate_normal(mean=[0, 0], cov=cov_matrix, size=n)\n",
    "X1 = X[:, 0]\n",
    "X2 = X[:, 1]\n",
    "\n",
    "# Generate Y\n",
    "z = 0.7*X1 + 0.7*X2 + np.random.normal(0, 1, n)\n",
    "prob = 1 / (1 + np.exp(-z))\n",
    "Y = (prob > 0.5).astype(int)\n",
    "\n",
    "# Fit models\n",
    "model1 = LogisticRegression(fit_intercept=False, max_iter=1000)\n",
    "model1.fit(X1.reshape(-1, 1), Y)\n",
    "\n",
    "model2 = LogisticRegression(fit_intercept=False, max_iter=1000)\n",
    "model2.fit(X2.reshape(-1, 1), Y)\n",
    "\n",
    "# Get predictions\n",
    "pred1 = model1.predict_proba(X1.reshape(-1, 1))[:, 1]\n",
    "pred2 = model2.predict_proba(X2.reshape(-1, 1))[:, 1]\n",
    "\n",
    "# Calculate correlations\n",
    "direct_corr, _ = pearsonr(X1, X2)\n",
    "pred_corr, _ = pearsonr(pred1, pred2)\n",
    "\n",
    "# Create visualization plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot original variables\n",
    "axes[0].scatter(X1, X2, c=Y, cmap='coolwarm', alpha=0.6)\n",
    "axes[0].set_title(f'X1 vs X2 (correlation: {direct_corr:.4f})')\n",
    "axes[0].set_xlabel('X1')\n",
    "axes[0].set_ylabel('X2')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "scatter = axes[1].scatter(pred1, pred2, c=Y, cmap='coolwarm', alpha=0.6)\n",
    "axes[1].set_title(f'Prediction from X1 vs Prediction from X2 (correlation: {pred_corr:.4f})')\n",
    "axes[1].set_xlabel('Prediction from X1')\n",
    "axes[1].set_ylabel('Prediction from X2')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "cbar.set_label('Y value (0 or 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the correlation values for this specific case\n",
    "print(f\"\\nFor input correlation of {direct_corr:.4f}:\")\n",
    "print(f\"Correlation between model predictions: {pred_corr:.4f}\")\n",
    "print(f\"Ratio (prediction corr / input corr): {pred_corr/direct_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Create correlated features and target\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "feature_A = np.random.normal(0, 1, n_samples)\n",
    "feature_B = feature_A + np.random.normal(0, 0.1, n_samples)  # ~0.9 correlation\n",
    "y = 2 * feature_A + np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "X = pd.DataFrame({'feature_A': feature_A, 'feature_B': feature_B})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Train XGBoost using sklearn API\n",
    "model = xgb.XGBRegressor(objective=\"reg:squarederror\", max_depth=3, learning_rate=0.1, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Use TreeExplainer (CPU-friendly)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Step 4: SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Step 5: Print average SHAP values\n",
    "mean_abs_shap = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Mean |SHAP|\": np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values(by=\"Mean |SHAP|\", ascending=False)\n",
    "\n",
    "print(mean_abs_shap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Create correlated features and target\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "feature_A = np.random.normal(0, 1, n_samples)\n",
    "feature_B = feature_A + np.random.normal(0, 0.1, n_samples)  # ~0.9 correlation\n",
    "y =  2* feature_A+ 2* feature_B + np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "X = pd.DataFrame({'feature_A': feature_A, 'feature_B': feature_B})\n",
    "XY = pd.DataFrame({'feature_A': feature_A, 'feature_B': feature_B, 'y': y})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Train XGBoost using sklearn API\n",
    "model = xgb.XGBRegressor(objective=\"reg:squarederror\", max_depth=3, learning_rate=0.1, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Use TreeExplainer (CPU-friendly)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY['y'].hist(bins=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XY.corr())\n",
    "# Convert SHAP values to DataFrame with matching column names\n",
    "shap_df = pd.DataFrame(shap_values, columns=X_test.columns)\n",
    "\n",
    "# Now you can compute the correlation matrix\n",
    "print(shap_df.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Print average SHAP values\n",
    "mean_abs_shap = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Mean |SHAP|\": np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values(by=\"Mean |SHAP|\", ascending=False)\n",
    "\n",
    "print(mean_abs_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from dython.nominal import associations,cluster_correlations\n",
    "\n",
    "# Load data \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert int classes to strings to allow associations \n",
    "# method to automatically recognize categorical columns\n",
    "target = ['C{}'.format(i) for i in iris.target]\n",
    "\n",
    "# Prepare data\n",
    "X = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "y = pd.DataFrame(data=target, columns=['target'])\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from dython.nominal import associations,cluster_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features associationsas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_correlations(associations(df,plot=False)['corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(df,plot=False)['corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['target']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# --------- NOMINAL VARIABLES ---------\n",
    "\n",
    "# Nominal variable 1: City (with 5 categories)\n",
    "cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "df['city'] = np.random.choice(cities, size=n)\n",
    "\n",
    "# Nominal variable 2: Product category (correlated with city)\n",
    "# Create a conditional probability matrix to establish correlation\n",
    "# Each row represents a city, each column a product category\n",
    "product_categories = ['Electronics', 'Clothing', 'Food', 'Books', 'Furniture']\n",
    "city_product_probs = {\n",
    "    'New York':    [0.5, 0.2, 0.1, 0.15, 0.05],  # New York has high electronics sales\n",
    "    'Los Angeles': [0.1, 0.6, 0.1, 0.1, 0.1],    # LA has high clothing sales\n",
    "    'Chicago':     [0.1, 0.1, 0.6, 0.1, 0.1],    # Chicago has high food sales\n",
    "    'Houston':     [0.1, 0.1, 0.1, 0.6, 0.1],    # Houston has high book sales\n",
    "    'Phoenix':     [0.1, 0.1, 0.1, 0.1, 0.6]     # Phoenix has high furniture sales\n",
    "}\n",
    "\n",
    "# Generate product categories based on city (creates correlation)\n",
    "df['product_category'] = df.apply(\n",
    "    lambda row: np.random.choice(product_categories, p=city_product_probs[row['city']]), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Nominal variable 3: Payment method (not correlated with other variables)\n",
    "payment_methods = ['Credit Card', 'Debit Card', 'PayPal', 'Cash', 'Gift Card']\n",
    "df['payment_method'] = np.random.choice(payment_methods, size=n)\n",
    "\n",
    "# --------- ORDINAL VARIABLES ---------\n",
    "\n",
    "# Ordinal variable 1: Customer satisfaction (1-5 scale)\n",
    "satisfaction_levels = [1, 2, 3, 4, 5]\n",
    "df['satisfaction'] = np.random.choice(satisfaction_levels, size=n)\n",
    "\n",
    "# Ordinal variable 2: Education level\n",
    "education_levels = ['High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
    "df['education'] = np.random.choice(education_levels, size=n)\n",
    "\n",
    "# --------- CONTINUOUS VARIABLES ---------\n",
    "\n",
    "# Continuous variable 1: Age (normal distribution)\n",
    "df['age'] = np.random.normal(40, 15, n)\n",
    "df['age'] = df['age'].clip(18, 90).round(0)  # Clip to reasonable age range\n",
    "\n",
    "# Continuous variable 2: Income - correlated with education level\n",
    "# Define base income means for each education level\n",
    "education_income_means = {\n",
    "    'High School': 30000,\n",
    "    'Associate': 45000,\n",
    "    'Bachelor': 70000,\n",
    "    'Master': 100000,\n",
    "    'PhD': 130000\n",
    "}\n",
    "\n",
    "# Generate income based on education with some noise\n",
    "df['income'] = df.apply(\n",
    "    lambda row: np.random.normal(education_income_means[row['education']], \n",
    "                                education_income_means[row['education']] * 0.2),\n",
    "    axis=1\n",
    ")\n",
    "df['income'] = df['income'].clip(20000, 200000).round(-2)  # Clip and round\n",
    "\n",
    "# Continuous variable 3: Purchase amount (continuous, in dollars)\n",
    "df['purchase_amount'] = np.random.gamma(5, 20, n)\n",
    "df['purchase_amount'] = df['purchase_amount'].round(2)\n",
    "\n",
    "# Continuous variable 4: Daily screen time (hours) - strongly correlated with product_category\n",
    "base_screen_time = {\n",
    "    'Electronics': 8,\n",
    "    'Clothing': 3,\n",
    "    'Food': 2,\n",
    "    'Books': 4,\n",
    "    'Furniture': 3\n",
    "}\n",
    "df['screen_time'] = df.apply(\n",
    "    lambda row: np.random.normal(base_screen_time[row['product_category']], 1.5),\n",
    "    axis=1\n",
    ")\n",
    "df['screen_time'] = df['screen_time'].clip(0, 16).round(1)  # Clip to reasonable range\n",
    "\n",
    "# Continuous variable 5: Credit score - not correlated with other variables\n",
    "df['credit_score'] = np.random.normal(700, 100, n)\n",
    "df['credit_score'] = df['credit_score'].clip(300, 850).round(0)\n",
    "\n",
    "# --------- DISCRETE VARIABLES ---------\n",
    "\n",
    "# Discrete variable 1: Number of purchases (counts) - \n",
    "# related to income (continuous) non-linearly\n",
    "df['num_purchases'] = df.apply(\n",
    "    lambda row: np.random.poisson(np.log(row['income']/10000)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Discrete variable 2: Household size (counts) - \n",
    "# Not strongly correlated with other variables\n",
    "df['household_size'] = np.random.poisson(3, n)\n",
    "df['household_size'] = df['household_size'].clip(1, 10)\n",
    "\n",
    "# --------- NON-LINEAR RELATIONSHIPS ---------\n",
    "\n",
    "# Create a non-linear relationship between age and health_score\n",
    "df['health_score'] = 100 - 0.01 * (df['age'] - 30) ** 2 + np.random.normal(0, 10, n)\n",
    "df['health_score'] = df['health_score'].clip(0, 100).round(1)\n",
    "\n",
    "# Create a non-linear relationship between income and vacation_days\n",
    "df['vacation_days'] = 5 + 20 * (1 - np.exp(-df['income']/50000)) + np.random.normal(0, 3, n)\n",
    "df['vacation_days'] = df['vacation_days'].clip(0, 45).round(0)\n",
    "\n",
    "# --------- FEATURE ENCODING FOR CORRELATION ANALYSIS ---------\n",
    "\n",
    "# Create a copy for visualization and correlation analysis\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# One-hot encode nominal variables\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['city', 'product_category', 'payment_method'])\n",
    "\n",
    "# Ordinal encode education level\n",
    "education_mapping = {'High School': 1, 'Associate': 2, 'Bachelor': 3, 'Master': 4, 'PhD': 5}\n",
    "df_encoded['education_encoded'] = df_encoded['education'].map(education_mapping)\n",
    "\n",
    "# Calculate and visualize correlation matrix for relevant numeric columns\n",
    "numeric_cols = ['age', 'income', 'purchase_amount', 'screen_time', 'credit_score', \n",
    "                'num_purchases', 'household_size', 'health_score', 'vacation_days', \n",
    "                'satisfaction', 'education_encoded']\n",
    "\n",
    "correlation_matrix = df_encoded[numeric_cols].corr()\n",
    "\n",
    "# Display first 10 rows of the dataframe\n",
    "print(\"Sample Dataframe (First 10 rows):\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix.round(2))\n",
    "\n",
    "# Explain the variable types and relationships\n",
    "print(\"\\nVariable Types:\")\n",
    "print(\"Nominal Variables: city, product_category, payment_method\")\n",
    "print(\"Ordinal Variables: satisfaction, education\")\n",
    "print(\"Continuous Variables: age, income, purchase_amount, screen_time, credit_score, health_score\")\n",
    "print(\"Discrete Variables: num_purchases, household_size, vacation_days\")\n",
    "\n",
    "print(\"\\nVariable Relationships:\")\n",
    "print(\"1. Nominal-Nominal correlations:\")\n",
    "print(\"   - city and product_category: Strong correlation (conditional probability)\")\n",
    "print(\"   - city and payment_method: No correlation (independent)\")\n",
    "\n",
    "print(\"\\n2. Continuous-Nominal correlations:\")\n",
    "print(\"   - screen_time and product_category: Strong correlation\")\n",
    "print(\"   - credit_score and product_category: No correlation\")\n",
    "\n",
    "print(\"\\n3. Continuous-Continuous correlations:\")\n",
    "print(\"   - income and education_encoded: Strong positive correlation\")\n",
    "print(\"   - credit_score and age: No correlation\")\n",
    "\n",
    "print(\"\\n4. Linear Relationships:\")\n",
    "print(\"   - income and education_encoded: Linear positive relationship\")\n",
    "print(\"   - screen_time and product_category: Linear relationship within categories\")\n",
    "\n",
    "print(\"\\n5. Non-Linear Relationships:\")\n",
    "print(\"   - age and health_score: Quadratic relationship (optimal health around age 30)\")\n",
    "print(\"   - income and vacation_days: Exponential relationship (diminishing returns)\")\n",
    "print(\"   - income and num_purchases: Logarithmic relationship\")\n",
    "\n",
    "# Return the final dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(df,num_num_assoc='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample size\n",
    "n = 500\n",
    "\n",
    "# Create a dataframe with three variables having non-linear relationships\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Variable 1: x - Independent variable (uniform distribution)\n",
    "df['x'] = np.linspace(0, 10, n)\n",
    "\n",
    "# Variable 2: y1 - Quadratic relationship with x\n",
    "# Formula: y1 = 2xÂ² - 5x + 3 + noise\n",
    "df['y1'] = 2 * df['x']**2 - 5 * df['x'] + 3 + np.random.normal(0, 5, n)\n",
    "\n",
    "# Variable 3: y2 - Sinusoidal relationship with x\n",
    "# Formula: y2 = 10 * sin(x) + noise\n",
    "df['y2'] = 10 * np.sin(df['x']) + np.random.normal(0, 2, n)\n",
    "\n",
    "# Display the first 10 rows of the dataframe\n",
    "print(\"Three-Variable Dataframe with Non-Linear Relationships (First 10 rows):\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Visualize the relationships\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the quadratic relationship\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(df['x'], df['y1'], alpha=0.6)\n",
    "plt.title('Quadratic Relationship: y1 = 2xÂ² - 5x + 3 + noise')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y1')\n",
    "\n",
    "# Plot the sinusoidal relationship\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(df['x'], df['y2'], alpha=0.6)\n",
    "plt.title('Sinusoidal Relationship: y2 = 10 * sin(x) + noise')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y2')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Calculate Pearson correlation (linear correlation)\n",
    "correlation_matrix = df.corr()\n",
    "print(\"\\nPearson Correlation Matrix (Linear Correlation):\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Spearman rank correlation (can detect monotonic non-linear relationships)\n",
    "spearman_corr = df.corr(method='spearman')\n",
    "print(\"\\nSpearman Rank Correlation Matrix:\")\n",
    "print(spearman_corr.round(3))\n",
    "\n",
    "# Calculate non-linear associations using correlation ratio (eta)\n",
    "def correlation_ratio(categories, measurements):\n",
    "    categories = np.asarray(categories)\n",
    "    measurements = np.asarray(measurements)\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat) + 1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0, cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array, n_array)) / np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array, np.power(np.subtract(y_avg_array, y_total_avg), 2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements, y_total_avg), 2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = np.sqrt(numerator / denominator)\n",
    "    return eta\n",
    "\n",
    "# Bin x into categories for the correlation ratio\n",
    "df['x_binned'] = pd.cut(df['x'], bins=10)\n",
    "\n",
    "# Calculate correlation ratios\n",
    "eta_x_y1 = correlation_ratio(df['x_binned'], df['y1'])\n",
    "eta_x_y2 = correlation_ratio(df['x_binned'], df['y2'])\n",
    "\n",
    "print(\"\\nNon-linear Association (Correlation Ratio Î·):\")\n",
    "print(f\"Correlation ratio between x and y1: {eta_x_y1:.3f}\")\n",
    "print(f\"Correlation ratio between x and y2: {eta_x_y2:.3f}\")\n",
    "\n",
    "print(\"\\nDescription of Non-Linear Relationships:\")\n",
    "print(\"1. x and y1: Quadratic relationship (parabola)\")\n",
    "print(\"   - Formula: y1 = 2xÂ² - 5x + 3 + random noise\")\n",
    "print(\"   - The relationship follows a U-shaped curve\")\n",
    "print(\"   - Pearson correlation is not effective at capturing this relationship\")\n",
    "print(\"   - Correlation ratio (Î·) shows a strong non-linear association\")\n",
    "\n",
    "print(\"\\n2. x and y2: Sinusoidal relationship\")\n",
    "print(\"   - Formula: y2 = 10 * sin(x) + random noise\")\n",
    "print(\"   - The relationship follows a wave pattern\")\n",
    "print(\"   - Pearson correlation is close to zero despite strong relationship\")\n",
    "print(\"   - Correlation ratio (Î·) better captures this non-linear pattern\")\n",
    "\n",
    "print(\"\\nImportant Note:\")\n",
    "print(\"Standard correlation coefficients (Pearson) mainly detect linear relationships.\")\n",
    "print(\"The correlation ratio (Î·) and other non-linear measures are better at identifying non-linear patterns.\")\n",
    "print(\"Visual inspection through scatter plots is often the best way to identify non-linear relationships.\")\n",
    "\n",
    "# Return the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(df,num_num_assoc='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import shap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample size\n",
    "n = 50000\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "###########################################\n",
    "# PART 1: CREATING THE VARIABLES\n",
    "###########################################\n",
    "\n",
    "# Create categorical variables that are correlated and explain target similarly\n",
    "# Categorical pair 1: correlated with each other and similarly predictive of target\n",
    "occupations = ['Engineer', 'Teacher', 'Doctor', 'Artist', 'Accountant']\n",
    "education_levels = ['Bachelor', 'Master', 'PhD', 'High School', 'Associate']\n",
    "\n",
    "# Create base probabilities for occupation\n",
    "occupation_probs = [0.25, 0.2, 0.15, 0.2, 0.2]\n",
    "df['occupation'] = np.random.choice(occupations, size=n, p=occupation_probs)\n",
    "\n",
    "# Create education with correlation to occupation\n",
    "occupation_edu_probs = {\n",
    "    'Engineer': [0.6, 0.3, 0.05, 0.025, 0.025],  # Engineers mostly have Bachelor's or Master's\n",
    "    'Teacher': [0.4, 0.4, 0.1, 0.05, 0.05],      # Teachers mostly have Bachelor's or Master's\n",
    "    'Doctor': [0.1, 0.3, 0.55, 0.025, 0.025],    # Doctors mostly have PhD or Master's\n",
    "    'Artist': [0.3, 0.1, 0.05, 0.5, 0.05],       # Artists mostly have Bachelor's or High School\n",
    "    'Accountant': [0.5, 0.2, 0.05, 0.05, 0.2]    # Accountants mostly have Bachelor's or Associate\n",
    "}\n",
    "\n",
    "df['education'] = df.apply(\n",
    "    lambda row: np.random.choice(education_levels, p=occupation_edu_probs[row['occupation']]), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create categorical variables that are uncorrelated but both predict target well\n",
    "# Categorical pair 2: uncorrelated with each other but both predictive of target\n",
    "device_types = ['Smartphone', 'Laptop', 'Tablet', 'Desktop', 'Smart Watch']\n",
    "payment_methods = ['Credit Card', 'PayPal', 'Debit Card', 'Cash', 'Bank Transfer']\n",
    "\n",
    "# Assign these independently (no correlation between them)\n",
    "df['device_type'] = np.random.choice(device_types, size=n)\n",
    "df['payment_method'] = np.random.choice(payment_methods, size=n)\n",
    "\n",
    "# Create continuous variables that are correlated and explain target similarly\n",
    "# Continuous pair 1: correlated with each other and similarly predictive of target\n",
    "df['income'] = np.random.normal(70000, 20000, n)\n",
    "# Create savings based on income (correlated)\n",
    "df['savings'] = 0.2 * df['income'] + np.random.normal(5000, 10000, n)\n",
    "df['savings'] = np.maximum(0, df['savings'])  # No negative savings\n",
    "\n",
    "# Create continuous variables that are uncorrelated but both predict target well\n",
    "# Continuous pair 2: uncorrelated with each other but both predictive of target\n",
    "df['age'] = np.random.normal(40, 12, n)\n",
    "df['age'] = np.clip(df['age'], 18, 80)\n",
    "\n",
    "df['purchase_frequency'] = np.random.gamma(5, 2, n)  # Independent from age\n",
    "\n",
    "###########################################\n",
    "# PART 2: CREATING THE TARGET VARIABLE\n",
    "###########################################\n",
    "\n",
    "# Now create a binary target variable that's influenced by all these predictors\n",
    "# But with different relationships (linear and non-linear)\n",
    "\n",
    "# Function to convert probabilities to binary outcomes\n",
    "def prob_to_binary(p):\n",
    "    return np.random.binomial(1, p)\n",
    "\n",
    "# Create target variable through a complex function of all predictors\n",
    "\n",
    "# 1. Effects from correlated categorical variables (occupation and education)\n",
    "occupation_target_probs = {\n",
    "    'Engineer': 0.7,\n",
    "    'Teacher': 0.6,\n",
    "    'Doctor': 0.75,\n",
    "    'Artist': 0.4,\n",
    "    'Accountant': 0.55\n",
    "}\n",
    "\n",
    "education_target_probs = {\n",
    "    'Bachelor': 0.6,\n",
    "    'Master': 0.7,\n",
    "    'PhD': 0.8,\n",
    "    'High School': 0.4,\n",
    "    'Associate': 0.5\n",
    "}\n",
    "\n",
    "cat_corr_effect = df.apply(\n",
    "    lambda row: (occupation_target_probs[row['occupation']] + \n",
    "                education_target_probs[row['education']]) / 2,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 2. Effects from uncorrelated categorical variables (device and payment)\n",
    "device_target_probs = {\n",
    "    'Smartphone': 0.6,\n",
    "    'Laptop': 0.7,\n",
    "    'Tablet': 0.5,\n",
    "    'Desktop': 0.65,\n",
    "    'Smart Watch': 0.45\n",
    "}\n",
    "\n",
    "payment_target_probs = {\n",
    "    'Credit Card': 0.65,\n",
    "    'PayPal': 0.7,\n",
    "    'Debit Card': 0.55,\n",
    "    'Cash': 0.4,\n",
    "    'Bank Transfer': 0.6\n",
    "}\n",
    "\n",
    "cat_uncorr_effect = df.apply(\n",
    "    lambda row: 0.4 * device_target_probs[row['device_type']] + \n",
    "                0.6 * payment_target_probs[row['payment_method']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 3. Effects from correlated continuous variables (income and savings)\n",
    "# Linear effect\n",
    "cont_corr_effect = 0.3 * ((df['income'] - df['income'].min()) / \n",
    "                         (df['income'].max() - df['income'].min())) + \\\n",
    "                  0.2 * ((df['savings'] - df['savings'].min()) / \n",
    "                         (df['savings'].max() - df['savings'].min()))\n",
    "\n",
    "# 4. Effects from uncorrelated continuous variables (age and purchase frequency)\n",
    "# Non-linear effect for age (quadratic - middle age has highest probability)\n",
    "age_normalized = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())\n",
    "age_effect = -4 * (age_normalized - 0.5)**2 + 1  # Peaks at age_normalized = 0.5\n",
    "\n",
    "# Linear effect for purchase frequency\n",
    "pf_normalized = (df['purchase_frequency'] - df['purchase_frequency'].min()) / \\\n",
    "                (df['purchase_frequency'].max() - df['purchase_frequency'].min())\n",
    "pf_effect = 0.8 * pf_normalized\n",
    "\n",
    "cont_uncorr_effect = 0.6 * age_effect + 0.4 * pf_effect\n",
    "\n",
    "# Combine all effects and convert to probability\n",
    "# Weight the effects from different variable groups\n",
    "total_effect = (0.25 * cat_corr_effect + \n",
    "                0.25 * cat_uncorr_effect + \n",
    "                0.25 * cont_corr_effect + \n",
    "                0.25 * cont_uncorr_effect)\n",
    "\n",
    "# Convert to binary target\n",
    "df['target_prob'] = total_effect\n",
    "df['target'] = df['target_prob'].apply(prob_to_binary)\n",
    "\n",
    "###########################################\n",
    "# PART 3: ANALYSIS OF RELATIONSHIPS\n",
    "###########################################\n",
    "\n",
    "# Encode categorical variables for correlation analysis\n",
    "df_encoded = df.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in ['occupation', 'education', 'device_type', 'payment_method']:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[f'{col}_encoded'] = le.fit_transform(df_encoded[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Calculate correlations\n",
    "corr_columns = ['occupation_encoded', 'education_encoded', 'device_type_encoded', \n",
    "                'payment_method_encoded', 'income', 'savings', 'age', \n",
    "                'purchase_frequency', 'target']\n",
    "\n",
    "corr_matrix = df_encoded[corr_columns].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Verify our intended relationships\n",
    "print(\"\\nVerifying intended relationships:\")\n",
    "\n",
    "# 1. Correlation between occupation and education (should be high)\n",
    "print(f\"Correlation between occupation and education: {corr_matrix.loc['occupation_encoded', 'education_encoded']:.3f}\")\n",
    "\n",
    "# 2. Correlation between device_type and payment_method (should be low)\n",
    "print(f\"Correlation between device_type and payment_method: {corr_matrix.loc['device_type_encoded', 'payment_method_encoded']:.3f}\")\n",
    "\n",
    "# 3. Correlation between income and savings (should be high)\n",
    "print(f\"Correlation between income and savings: {corr_matrix.loc['income', 'savings']:.3f}\")\n",
    "\n",
    "# 4. Correlation between age and purchase_frequency (should be low)\n",
    "print(f\"Correlation between age and purchase_frequency: {corr_matrix.loc['age', 'purchase_frequency']:.3f}\")\n",
    "\n",
    "# 5. Correlation with target for all variables\n",
    "print(\"\\nCorrelation with target:\")\n",
    "for col in corr_columns[:-1]:  # Exclude target itself\n",
    "    print(f\"{col}: {corr_matrix.loc[col, 'target']:.3f}\")\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"\\nTarget distribution: {df['target'].value_counts(normalize=True)}\")\n",
    "\n",
    "###########################################\n",
    "# PART 4: TRAIN-TEST SPLIT AND XGBOOST MODEL\n",
    "###########################################\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = df_encoded.drop(['target', 'target_prob'], axis=1)\n",
    "y = df_encoded['target']\n",
    "\n",
    "# We'll need to one-hot encode the categorical variables for XGBoost\n",
    "X_processed = pd.get_dummies(X, columns=['occupation', 'education', 'device_type', 'payment_method'], drop_first=True)\n",
    "\n",
    "# Remove the encoded columns that we created just for correlation analysis\n",
    "X_processed = X_processed.drop(['occupation_encoded', 'education_encoded', \n",
    "                              'device_type_encoded', 'payment_method_encoded'], axis=1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features by Importance:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "###########################################\n",
    "# PART 5: SHAP VALUES FOR FEATURE IMPORTANCE\n",
    "###########################################\n",
    "\n",
    "# Calculate SHAP values\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Print mean absolute SHAP values for top 10 features\n",
    "mean_shap = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'SHAP_abs': np.abs(shap_values.values).mean(0)\n",
    "})\n",
    "mean_shap = mean_shap.sort_values('SHAP_abs', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features by Mean Absolute SHAP Value:\")\n",
    "print(mean_shap.head(10))\n",
    "\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(f\"1. Categorical variables that are correlated and explain target similarly: occupation, education\")\n",
    "print(f\"2. Categorical variables that are uncorrelated but both explain target: device_type, payment_method\")\n",
    "print(f\"3. Continuous variables that are correlated and explain target similarly: income, savings\")\n",
    "print(f\"4. Continuous variables that are uncorrelated but both explain target: age, purchase_frequency\")\n",
    "\n",
    "print(\"\\nResults explanation:\")\n",
    "print(\"1. The SHAP values show which features are most important for the model's predictions.\")\n",
    "print(\"2. Higher SHAP values indicate stronger impact on the model output.\")\n",
    "print(\"3. The direction of SHAP values (positive/negative) shows whether a feature increases or decreases the prediction.\")\n",
    "\n",
    "# Return the first few rows of the dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(columns=[ 'target_prob'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_features=['occupation', 'education', 'device_type', 'payment_method']\n",
    "num_features=['income', 'savings', 'age', 'purchase_frequency']\n",
    "selected_features=category_features+num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[category_features] = df[category_features].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final[['x1','x3']]\n",
    "y = df_final['target']\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with scikit-learn API\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method='hist',\n",
    "    scale_pos_weight=sum(y_train==0)/sum(y_train==1),\n",
    "    enable_categorical=True,  # Enable categorical features\n",
    "    random_state=42,\n",
    "   # early_stopping_rounds=20,\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],  \n",
    "    verbose=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Print average SHAP values\n",
    "mean_abs_shap = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Mean |SHAP|\": np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values(by=\"Mean |SHAP|\", ascending=False)\n",
    "\n",
    "print(mean_abs_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Print average SHAP values\n",
    "mean_abs_shap = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Mean |SHAP|\": np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values(by=\"Mean |SHAP|\", ascending=False)\n",
    "\n",
    "print(mean_abs_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SHAP values into DataFrame\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(shap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "print(\"Starting dataset generation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define parameters\n",
    "n_samples = 300000  # 3 lakh records\n",
    "n_features = 1000  # Total features\n",
    "n_informative = 20  # Features with strong predictive relationship\n",
    "n_redundant = 10  # Correlated features\n",
    "n_categorical = 200  # Number of categorical features\n",
    "\n",
    "# Generate the base dataset with numeric features\n",
    "print(\"Generating base classification dataset...\")\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    n_redundant=n_redundant,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    class_sep=1.0,  # Increase separation for stronger predictive relationship\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "print(\"Converting to DataFrame...\")\n",
    "column_names = [f'num_{i}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "# Add binary target column\n",
    "df['target'] = y\n",
    "\n",
    "# Convert some numeric features to categorical\n",
    "print(\"Converting some features to categorical...\")\n",
    "categorical_indices = np.random.choice(n_features, n_categorical, replace=False)\n",
    "categorical_cols = []\n",
    "\n",
    "for idx in categorical_indices:\n",
    "    col_name = f'num_{idx}'\n",
    "    categorical_cols.append(col_name)\n",
    "    \n",
    "    # Determine number of categories (between 3 and 10)\n",
    "    n_categories = np.random.randint(3, 11)\n",
    "    \n",
    "    # Convert numeric to categorical\n",
    "    df[col_name] = pd.qcut(\n",
    "        df[col_name], \n",
    "        q=n_categories, \n",
    "        labels=[f'cat_{j}' for j in range(n_categories)],\n",
    "        duplicates='drop'\n",
    "    )\n",
    "    \n",
    "    # Convert to category dtype\n",
    "    df[col_name] = df[col_name].astype('category')\n",
    "\n",
    "# Rename categorical columns\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    new_name = f'cat_{i}'\n",
    "    df = df.rename(columns={col: new_name})\n",
    "\n",
    "# Add some datetime features\n",
    "print(\"Adding datetime features...\")\n",
    "base_date = pd.Timestamp('2020-01-01')\n",
    "date_range = pd.date_range(start=base_date, periods=n_samples, freq='30s')\n",
    "df['date_feature'] = date_range\n",
    "df['month'] = df['date_feature'].dt.month.astype('category')\n",
    "df['day'] = df['date_feature'].dt.day.astype('category')\n",
    "df['hour'] = df['date_feature'].dt.hour.astype('category')\n",
    "\n",
    "# Add some ID-like features\n",
    "print(\"Adding ID features...\")\n",
    "df['id'] = np.arange(n_samples)\n",
    "df['uuid'] = [f'uuid-{i:09d}' for i in range(n_samples)]\n",
    "\n",
    "# Generate dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Number of samples: {n_samples}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")  # excluding target\n",
    "print(f\"Number of categorical features: {len(categorical_cols) + 3}\")  # +3 for month, day, hour\n",
    "print(f\"Target distribution:\\n{df['target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Display memory usage\n",
    "memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "print(f\"Dataset memory usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Check for null values\n",
    "print(f\"\\nNull values in dataset: {df.isnull().sum().sum()}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nTotal time to generate dataset: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Save a small sample for demonstration\n",
    "sample_df = df.sample(n=min(1000, n_samples), random_state=42)\n",
    "sample_df.to_csv('synthetic_dataset_sample.csv', index=False)\n",
    "print(\"Saved sample to 'synthetic_dataset_sample.csv'\")\n",
    "\n",
    "# Return the DataFrame\n",
    "print(\"Dataset generation complete!\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in credit_risk_df.select_dtypes(include='object').columns:\n",
    "    credit_risk_df[col] = credit_risk_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_risk_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_risk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit_risk_df.loc[:, 'income':'application_channel']\n",
    "y = credit_risk_df['default']\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_uv.func import shap_feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features,importance_df =shap_feature_selection(train_data=credit_risk_df,feature_names=X.columns,target_name='default',verbose=True,\n",
    "                                                        test_size=0.3,random_state=42,use_train_for_shap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=importance_df[importance_df['Cumulative_Importance']<0.9999]['Feature'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = associations(credit_risk_df[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 1s (self-correlations) with NaN\n",
    "corr_matrix_no_diag = c['corr'].where(~np.eye(c['corr'].shape[0], dtype=bool))\n",
    "\n",
    "# Find max and min correlation values\n",
    "max_corr = corr_matrix_no_diag.max().max()\n",
    "min_corr = corr_matrix_no_diag.min().min()\n",
    "\n",
    "print(f\"Max correlation (excluding 1.0): {max_corr}\")\n",
    "print(f\"Min correlation: {min_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df.loc[:, 'num_0':'num_999']\n",
    "X = credit_risk_df[f]\n",
    "y = credit_risk_df['default']\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[f], y, test_size=0.4, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with scikit-learn API\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method='hist',\n",
    "    scale_pos_weight=sum(y_train==0)/sum(y_train==1),\n",
    "    enable_categorical=True,  # Enable categorical features\n",
    "    random_state=42,\n",
    "   # early_stopping_rounds=20,\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],  \n",
    "    verbose=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Print average SHAP values\n",
    "mean_abs_shap = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Mean |SHAP|\": np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values(by=\"Mean |SHAP|\", ascending=False)\n",
    "\n",
    "print(mean_abs_shap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SHAP values into DataFrame\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.feature_names)\n",
    "cc=associations(shap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc['corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_values.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 1s (self-correlations) with NaN\n",
    "corr_matrix_no_diag = cc['corr'].where(~np.eye(cc['corr'].shape[0], dtype=bool))\n",
    "\n",
    "# Find max and min correlation values\n",
    "max_corr = corr_matrix_no_diag.max().max()\n",
    "min_corr = corr_matrix_no_diag.min().min()\n",
    "\n",
    "print(f\"Max correlation (excluding 1.0): {max_corr}\")\n",
    "print(f\"Min correlation: {min_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_correlations(associations(shap_df,plot=False)['corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([11, 26,  4, 14, 20, 24, 16, 22,  1,  6, 25, 13,  3, 21, 17,  9,  7,\n",
    "         8, 12, 23, 27, 18, 15,  5, 29, 28, 10, 19,  2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_credit_risk_data(n_samples=50000, default_rate=0.10):\n",
    "    \"\"\"\n",
    "    Generate synthetic credit risk data with specified characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of data points to generate\n",
    "    - default_rate: Proportion of default cases (target=1)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with features and target variable\n",
    "    \"\"\"\n",
    "    # Calculate number of defaults\n",
    "    n_defaults = int(n_samples * default_rate)\n",
    "    n_non_defaults = n_samples - n_defaults\n",
    "    \n",
    "    # Generate base data with 10 informative features\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=10,  # We'll expand to 50 later\n",
    "        n_informative=8,  # Truly informative\n",
    "        n_redundant=2,   # Correlated features\n",
    "        n_classes=2,\n",
    "        weights=[1-default_rate, default_rate],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    feature_names = [f\"feature_{i+1}\" for i in range(X.shape[1])]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    \n",
    "    # Rename the first 10 features to meaningful names\n",
    "    # Highly predictive numeric variables (6)\n",
    "    df.rename(columns={\n",
    "        'feature_1': 'income',\n",
    "        'feature_2': 'debt_to_income_ratio',\n",
    "        'feature_3': 'credit_score',\n",
    "        'feature_4': 'loan_amount', \n",
    "        'feature_5': 'interest_rate',\n",
    "        'feature_6': 'age',\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Create a highly correlated feature with income\n",
    "    df['total_assets'] = df['income'] * np.random.normal(3, 0.2, n_samples) + np.random.normal(0, 0.5, n_samples)\n",
    "    \n",
    "    # Transform features to more realistic ranges\n",
    "    df['income'] = (df['income'] * 20000 + 50000).clip(10000, 250000)\n",
    "    df['debt_to_income_ratio'] = (df['debt_to_income_ratio'] + 3) / 6  # Scale to 0-1 range\n",
    "    df['debt_to_income_ratio'] = (df['debt_to_income_ratio'] * 0.6 + 0.1).clip(0.05, 0.8)  \n",
    "    df['credit_score'] = (df['credit_score'] * 150 + 650).clip(300, 850).astype(int)\n",
    "    df['loan_amount'] = (df['loan_amount'] * 50000 + 100000).clip(5000, 500000)\n",
    "    df['interest_rate'] = (df['interest_rate'] + 3) / 6 * 15 + 2  # Interest rate between 2% and 17%\n",
    "    df['age'] = (df['age'] * 20 + 40).clip(18, 85).astype(int)\n",
    "    \n",
    "    # Highly predictive categorical variables (4)\n",
    "    # Feature 7: employment_status (categorical)\n",
    "    employment_statuses = ['Employed', 'Self-Employed', 'Unemployed', 'Retired']\n",
    "    # Make employment more predictive - separate probability arrays for default vs non-default\n",
    "    employed_prob_default = [0.3, 0.2, 0.4, 0.1]\n",
    "    employed_prob_nondefault = [0.7, 0.2, 0.05, 0.05]\n",
    "    \n",
    "    # Create array to store employment status\n",
    "    employment_status = []\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:\n",
    "            employment_status.append(np.random.choice(employment_statuses, p=employed_prob_default))\n",
    "        else:\n",
    "            employment_status.append(np.random.choice(employment_statuses, p=employed_prob_nondefault))\n",
    "    \n",
    "    df['employment_status'] = employment_status\n",
    "    \n",
    "    # Feature 8: education_level (categorical)\n",
    "    education_levels = ['High School', 'Bachelor', 'Master', 'PhD', 'Other']\n",
    "    # Make education more predictive - separate probability arrays\n",
    "    education_prob_default = [0.5, 0.3, 0.1, 0.05, 0.05]\n",
    "    education_prob_nondefault = [0.2, 0.4, 0.3, 0.08, 0.02]\n",
    "    \n",
    "    # Create array to store education level\n",
    "    education_level = []\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:\n",
    "            education_level.append(np.random.choice(education_levels, p=education_prob_default))\n",
    "        else:\n",
    "            education_level.append(np.random.choice(education_levels, p=education_prob_nondefault))\n",
    "    \n",
    "    df['education_level'] = education_level\n",
    "    \n",
    "    # Feature 9: loan_purpose (categorical)\n",
    "    loan_purposes = ['Home', 'Auto', 'Education', 'Personal', 'Business', 'Debt Consolidation']\n",
    "    # Make loan_purpose more predictive\n",
    "    purpose_prob_default = [0.1, 0.15, 0.2, 0.2, 0.25, 0.1]\n",
    "    purpose_prob_nondefault = [0.3, 0.2, 0.1, 0.1, 0.1, 0.2]\n",
    "    \n",
    "    # Create array to store loan purpose\n",
    "    loan_purpose = []\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:\n",
    "            loan_purpose.append(np.random.choice(loan_purposes, p=purpose_prob_default))\n",
    "        else:\n",
    "            loan_purpose.append(np.random.choice(loan_purposes, p=purpose_prob_nondefault))\n",
    "    \n",
    "    df['loan_purpose'] = loan_purpose\n",
    "    \n",
    "    # Feature 10: has_previous_defaults (categorical but binary)\n",
    "    # Make previous defaults highly predictive\n",
    "    has_previous_defaults = []\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 1:\n",
    "            # 70% of defaulters have previous defaults\n",
    "            has_previous_defaults.append(np.random.choice([1, 0], p=[0.7, 0.3]))\n",
    "        else:\n",
    "            # 10% of non-defaulters have previous defaults\n",
    "            has_previous_defaults.append(np.random.choice([1, 0], p=[0.1, 0.9]))\n",
    "    \n",
    "    df['has_previous_defaults'] = [('Yes' if val == 1 else 'No') for val in has_previous_defaults]\n",
    "    \n",
    "    # Create a correlated categorical variable with employment_status\n",
    "    # payment_history (correlated with employment_status)\n",
    "    payment_history_map = {\n",
    "        'Employed': np.array(['Excellent', 'Good', 'Fair', 'Poor']),\n",
    "        'Self-Employed': np.array(['Good', 'Fair', 'Fair', 'Poor']),\n",
    "        'Unemployed': np.array(['Fair', 'Poor', 'Poor', 'Poor']), \n",
    "        'Retired': np.array(['Excellent', 'Good', 'Fair', 'Poor'])\n",
    "    }\n",
    "    \n",
    "    # Create probabilities for payment_history based on employment status\n",
    "    payment_probs = {\n",
    "        'Employed': [0.5, 0.3, 0.15, 0.05],\n",
    "        'Self-Employed': [0.3, 0.4, 0.2, 0.1],\n",
    "        'Unemployed': [0.1, 0.2, 0.3, 0.4],\n",
    "        'Retired': [0.4, 0.3, 0.2, 0.1]\n",
    "    }\n",
    "    \n",
    "    payment_history = []\n",
    "    for status in df['employment_status']:\n",
    "        payment_history.append(np.random.choice(payment_history_map[status], p=payment_probs[status]))\n",
    "    \n",
    "    df['payment_history'] = payment_history\n",
    "    \n",
    "    # Add remaining 40 less predictive features (mix of numeric and categorical)\n",
    "    # Numeric features (30)\n",
    "    for i in range(1, 31):\n",
    "        # Generate less predictive numeric features\n",
    "        feature_name = f'numeric_feature_{i}'\n",
    "        if i <= 5:  # First 5 slightly more predictive than the rest\n",
    "            feature_values = np.random.normal(0, 1, n_samples) + y * np.random.uniform(0.1, 0.3)\n",
    "        else:  # Remaining 25 features are mostly noise\n",
    "            feature_values = np.random.normal(0, 1, n_samples) + y * np.random.uniform(0, 0.1)\n",
    "            \n",
    "        # Apply different transformations to make features diverse\n",
    "        if i % 4 == 0:\n",
    "            # Exponential-like features (e.g., transaction amounts)\n",
    "            feature_values = np.exp(feature_values * 0.5) * 100\n",
    "        elif i % 4 == 1:\n",
    "            # Percentage-like features (e.g., utilization rates)\n",
    "            feature_values = stats.norm.cdf(feature_values) * 100\n",
    "        elif i % 4 == 2:\n",
    "            # Count-like features (e.g., number of inquiries)\n",
    "            feature_values = np.abs(feature_values * 5).astype(int)\n",
    "        # else leave as standard normal\n",
    "            \n",
    "        df[feature_name] = feature_values\n",
    "    \n",
    "    # Categorical features (10)\n",
    "    categorical_vars = [\n",
    "        ('marital_status', ['Single', 'Married', 'Divorced', 'Widowed']),\n",
    "        ('housing_status', ['Own', 'Mortgage', 'Rent', 'Other']),\n",
    "        ('job_industry', ['Technology', 'Healthcare', 'Finance', 'Education', 'Manufacturing', 'Retail', 'Other']),\n",
    "        ('state', ['CA', 'NY', 'TX', 'FL', 'IL', 'PA', 'OH', 'GA', 'Other']),\n",
    "        ('credit_card_type', ['Visa', 'Mastercard', 'Amex', 'Discover', 'None']),\n",
    "        ('num_dependents', [0, 1, 2, 3, 4, '5+']),\n",
    "        ('months_at_current_job', ['<6', '6-12', '1-3 years', '3-5 years', '5+ years']),\n",
    "        ('has_cosigner', ['Yes', 'No']),\n",
    "        ('account_type', ['Checking', 'Savings', 'Both', 'None']),\n",
    "        ('application_channel', ['Online', 'In-person', 'Phone', 'Mail'])\n",
    "    ]\n",
    "    \n",
    "    for i, (feature_name, categories) in enumerate(categorical_vars):\n",
    "        # For first 3 categorical variables, make them slightly predictive\n",
    "        if i < 3:\n",
    "            # Different probability distributions based on target\n",
    "            p_default = np.random.dirichlet(np.ones(len(categories)) * 2)\n",
    "            p_non_default = np.random.dirichlet(np.ones(len(categories)) * 2)\n",
    "            \n",
    "            # Ensure some difference between distributions\n",
    "            max_idx = np.argmax(p_default)\n",
    "            p_default[max_idx] += 0.1\n",
    "            p_default = p_default / sum(p_default)\n",
    "            \n",
    "            # Choose categories based on target\n",
    "            cat_values = []\n",
    "            for target_val in y:\n",
    "                if target_val == 1:\n",
    "                    cat_values.append(np.random.choice(categories, p=p_default))\n",
    "                else:\n",
    "                    cat_values.append(np.random.choice(categories, p=p_non_default))\n",
    "        else:\n",
    "            # For the rest, almost no predictive power\n",
    "            cat_values = np.random.choice(categories, size=n_samples)\n",
    "            \n",
    "        df[feature_name] = cat_values\n",
    "    \n",
    "    # Add target variable\n",
    "    df['default'] = y\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "credit_risk_df = generate_credit_risk_data(n_samples=50000, default_rate=0.10)\n",
    "\n",
    "# Verify the default rate\n",
    "default_rate = credit_risk_df['default'].mean()\n",
    "print(f\"Default rate in the dataset: {default_rate:.4f}\")\n",
    "\n",
    "# Check correlation between income and total_assets (should be highly correlated)\n",
    "income_assets_corr = credit_risk_df['income'].corr(credit_risk_df['total_assets'])\n",
    "print(f\"Correlation between income and total_assets: {income_assets_corr:.4f}\")\n",
    "\n",
    "# Check correlation between employment_status and payment_history (categorical correlation)\n",
    "crosstab = pd.crosstab(credit_risk_df['employment_status'], credit_risk_df['payment_history'])\n",
    "print(\"\\nCrosstab of employment_status and payment_history:\")\n",
    "print(crosstab)\n",
    "\n",
    "# Show information about the dataset\n",
    "print(\"\\nDataset information:\")\n",
    "print(f\"Total rows: {len(credit_risk_df)}\")\n",
    "print(f\"Total columns: {len(credit_risk_df.columns)}\")\n",
    "print(f\"Default cases: {credit_risk_df['default'].sum()}\")\n",
    "print(f\"Non-default cases: {len(credit_risk_df) - credit_risk_df['default'].sum()}\")\n",
    "\n",
    "# Show a sample of the data\n",
    "print(\"\\nSample of the generated dataset:\")\n",
    "print(credit_risk_df.head())\n",
    "\n",
    "# Feature importance analysis\n",
    "# We'll use correlation for numeric features and chi-square for categorical\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Analyze numeric features\n",
    "numeric_features = credit_risk_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_features = [col for col in numeric_features if col != 'default']\n",
    "\n",
    "print(\"\\nCorrelation of numeric features with default:\")\n",
    "correlations = {}\n",
    "for col in numeric_features:\n",
    "    corr = credit_risk_df[col].corr(credit_risk_df['default'])\n",
    "    correlations[col] = abs(corr)\n",
    "\n",
    "sorted_numeric = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "for feature, corr in sorted_numeric[:15]:  # Show top 15\n",
    "    print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Analyze categorical features\n",
    "categorical_features = credit_risk_df.select_dtypes(include=['object']).columns\n",
    "print(\"\\nChi-square statistics for categorical features:\")\n",
    "chi2_values = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    contingency = pd.crosstab(credit_risk_df[col], credit_risk_df['default'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    chi2_values[col] = (chi2, p)\n",
    "\n",
    "sorted_categorical = sorted(chi2_values.items(), key=lambda x: x[1][0], reverse=True)\n",
    "for feature, (chi2, p) in sorted_categorical:\n",
    "    print(f\"{feature}: Chi2={chi2:.2f}, p-value={p:.6f}\")\n",
    "\n",
    "# Save to CSV\n",
    "credit_risk_df.to_csv('credit_risk_dataset.csv', index=False)\n",
    "print(\"\\nDataset saved to 'credit_risk_dataset.csv'\")\n",
    "\n",
    "# Summary of highly predictive features\n",
    "print(\"\\nHighly predictive features:\")\n",
    "print(\"Numeric: income, debt_to_income_ratio, credit_score, loan_amount, interest_rate, age\")\n",
    "print(\"Correlated numeric pair: income and total_assets\")\n",
    "print(\"Categorical: employment_status, education_level, loan_purpose, has_previous_defaults\")\n",
    "print(\"Correlated categorical pair: employment_status and payment_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_risk_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations(credit_risk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_correlations(associations(credit_risk_df,plot=False)['corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([4, 1, 1, 2, 2, 6, 1, 2, 5, 4, 4, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5,\n",
    "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "        5, 5, 5, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,      # Informative features\n",
    "    n_redundant=5,        # Redundant features (linear combinations of informative)\n",
    "    n_classes=2,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "# Create feature names like \"feature_0\", \"feature_1\", ..., \"feature_9\"\n",
    "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "X = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Optionally convert target to Series\n",
    "y = pd.Series(y, name=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= pd.DataFrame(X)\n",
    "y= pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a mask to avoid duplicate pairs and self-correlation\n",
    "mask = np.triu(np.ones(corrmat.shape), k=1).astype(bool)\n",
    "high_corr_pairs = (\n",
    "    corrmat.where(mask)\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"correlation\", \"level_0\": \"feature_1\", \"level_1\": \"feature_2\"})\n",
    ")\n",
    "\n",
    "# Filter strong correlations\n",
    "high_corr_pairs = high_corr_pairs[abs(high_corr_pairs[\"correlation\"]) > 0.80]\n",
    "print(high_corr_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "\n",
    "def find_correlation_groups(\n",
    "    X: pd.DataFrame,\n",
    "    corr_threshold: float = 0.8\n",
    ") -> Dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Find groups of correlated features based on a correlation threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        DataFrame containing only numeric feature columns.\n",
    "    corr_threshold : float, optional (default=0.8)\n",
    "        Absolute correlation threshold above which features are considered correlated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[int, List[str]]\n",
    "        Dictionary where each key is a group ID and each value is a list of correlated feature names.\n",
    "        Features not correlated with any other above the threshold are returned in individual groups.\n",
    "    \"\"\"\n",
    "    corrmat = X.corr().abs()\n",
    "    corr_pairs = corrmat.unstack()\n",
    "    filtered_pairs = corr_pairs[\n",
    "        (corr_pairs > corr_threshold) & (corr_pairs < 1)\n",
    "    ].reset_index()\n",
    "    filtered_pairs.columns = ['feature1', 'feature2', 'corr']\n",
    "\n",
    "    correlation_groups: Dict[int, Set[str]] = defaultdict(set)\n",
    "    features_assigned: Set[str] = set()\n",
    "\n",
    "    # First pass: create groups based on correlated pairs\n",
    "    for _, row in filtered_pairs.iterrows():\n",
    "        f1, f2 = row['feature1'], row['feature2']\n",
    "        group_found = False\n",
    "        for group in correlation_groups.values():\n",
    "            if f1 in group or f2 in group:\n",
    "                group.update([f1, f2])\n",
    "                group_found = True\n",
    "                break\n",
    "        if not group_found:\n",
    "            group_id = len(correlation_groups)\n",
    "            correlation_groups[group_id] = {f1, f2}\n",
    "        features_assigned.update([f1, f2])\n",
    "\n",
    "    # Second pass: merge overlapping groups\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged = False\n",
    "        keys = list(correlation_groups.keys())\n",
    "        for i in range(len(keys)):\n",
    "            for j in range(i + 1, len(keys)):\n",
    "                g1, g2 = keys[i], keys[j]\n",
    "                if g1 in correlation_groups and g2 in correlation_groups:\n",
    "                    if correlation_groups[g1] & correlation_groups[g2]:\n",
    "                        correlation_groups[g1].update(correlation_groups[g2])\n",
    "                        del correlation_groups[g2]\n",
    "                        merged = True\n",
    "                        break\n",
    "            if merged:\n",
    "                break\n",
    "\n",
    "    # Add non-correlated features\n",
    "    ungrouped_features = set(X.columns) - features_assigned\n",
    "    for feature in ungrouped_features:\n",
    "        correlation_groups[len(correlation_groups)] = {feature}\n",
    "\n",
    "    return {k: sorted(list(v)) for k, v in correlation_groups.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_groups = find_correlation_groups(X_train, corr_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data manually\n",
    "impdata = [\n",
    "    {\"feature_name\": \"feature_4\", \"importance\": 25},\n",
    "    {\"feature_name\": \"feature_5\", \"importance\": 30},\n",
    "    {\"feature_name\": \"feature_8\", \"importance\": 35}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "impdata = pd.DataFrame(impdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def select_best_features_from_groups(correlated_groups, feature_importance_df,\n",
    "                                     feature_name_col='feature_name',\n",
    "                                     feature_importance_col='importance'):\n",
    "    \"\"\"\n",
    "    Select the best feature from each correlation group based on feature importance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    correlated_groups : dict\n",
    "        Dictionary where keys are group IDs and values are lists of feature names\n",
    "    feature_importance_df : pandas DataFrame\n",
    "        DataFrame with at least two columns for feature names and importance values\n",
    "    feature_name_col : str\n",
    "        Name of the feature column to be used for importance comparison\n",
    "    feature_importance_col : str\n",
    "        Name of the importance column to be used for comparison\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    result_df : pandas DataFrame\n",
    "        DataFrame with columns: 'feature', 'group', 'importance', 'keep'\n",
    "    selected_features : list\n",
    "        List of features to keep\n",
    "    \"\"\"\n",
    "    # Create a dictionary for quick lookup of feature importance\n",
    "    importance_dict = pd.Series(\n",
    "        feature_importance_df[feature_importance_col].values,\n",
    "        index=feature_importance_df[feature_name_col]\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Create a list to hold rows for the result dataframe\n",
    "    result_rows = []\n",
    "    \n",
    "    # Process each correlation group\n",
    "    for group_id, features in correlated_groups.items():\n",
    "        # Get importance for each feature in the group\n",
    "        group_features_data = [(f, importance_dict.get(f, float('nan'))) for f in features]\n",
    "        \n",
    "        # For groups with multiple features, find the one with highest importance\n",
    "        if len(features) > 1:\n",
    "            # Find feature with max importance\n",
    "            best_feature, _ = max(group_features_data, key=lambda x: x[1])\n",
    "        else:\n",
    "            # If only one feature, keep it\n",
    "            best_feature = features[0]\n",
    "        \n",
    "        # Add all features from this group to results\n",
    "        for feature, importance in group_features_data:\n",
    "            result_rows.append({\n",
    "                'feature': feature,\n",
    "                'group': group_id,\n",
    "                'importance': importance,\n",
    "                'keep': feature == best_feature\n",
    "            })\n",
    "    \n",
    "    # Create result dataframe from rows\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    \n",
    "    # Sort by group and importance (descending)\n",
    "    if not result_df.empty:\n",
    "        result_df = result_df.sort_values(['group', 'importance'], ascending=[True, False])\n",
    "    \n",
    "    # Get list of features to keep\n",
    "    selected_features = result_df.loc[result_df['keep'], 'feature'].tolist()\n",
    "    \n",
    "    return result_df, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " result_df, selected_features = select_best_features_from_groups(\n",
    "     correlated_groups, impdata)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "\n",
    "# Create two uncorrelated predictors\n",
    "x1 = np.random.normal(0, 1, n)\n",
    "x2 = np.random.normal(0, 1, n)\n",
    "\n",
    "# Ensure they are uncorrelated by generating independent random variables\n",
    "# We'll check correlation later to confirm\n",
    "\n",
    "# Create target variable that depends equally on both predictors\n",
    "# Using a logistic function to create binary outcomes\n",
    "# z = 0.5*x1 + 0.5*x2 + noise\n",
    "z = 0.5 * x1 + 0.5 * x2 + np.random.normal(0, 0.5, n)\n",
    "probabilities = 1 / (1 + np.exp(-z))  # Sigmoid function\n",
    "y = (probabilities > 0.5).astype(int)  # Binary outcome\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'predictor1': x1,\n",
    "    'predictor2': x2,\n",
    "    'target': y\n",
    "})\n",
    "\n",
    "# Check correlation between predictors\n",
    "correlation = np.corrcoef(x1, x2)[0, 1]\n",
    "print(f\"Correlation between predictor1 and predictor2: {correlation:.6f}\")\n",
    "\n",
    "# Verify equal contribution using logistic regression\n",
    "X = df[['predictor1', 'predictor2']]\n",
    "y = df['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"\\nLogistic Regression Coefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_[0]):\n",
    "    print(f\"{feature}: {coef:.6f}\")\n",
    "\n",
    "# Model performance\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Scatter plot of predictors colored by target\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.scatterplot(x='predictor1', y='predictor2', hue='target', data=df, alpha=0.6)\n",
    "plt.title('Scatter Plot: Predictor1 vs Predictor2')\n",
    "\n",
    "# Plot 2: Distribution of predictor1 by target class\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(data=df, x='predictor1', hue='target', bins=30, kde=True, element=\"step\")\n",
    "plt.title('Distribution of Predictor1 by Target Class')\n",
    "\n",
    "# Plot 3: Distribution of predictor2 by target class\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(data=df, x='predictor2', hue='target', bins=30, kde=True, element=\"step\")\n",
    "plt.title('Distribution of Predictor2 by Target Class')\n",
    "\n",
    "# Plot 4: Correlation heatmap\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Preview the dataset\n",
    "print(\"\\nDataset Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scorecardutils.feature_selection import shap_feature_selection,find_correlation_groups,select_best_features_from_corr_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define target and features names\n",
    "target = 'target'\n",
    "features = df.drop(columns=[target]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features,importance_df,_ =shap_feature_selection(train_data=df,feature_names=features,target_name=target,verbose=True,\n",
    "                                                        test_size=0.3,random_state=42,use_train_for_shap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,importance_df,shapDF =shap_feature_selection(train_data=df,feature_names=selected_features,target_name=target,\n",
    "                                               verbose=False,\n",
    "                                                        split_data=False,random_state=42,\n",
    "                                                        create_shap_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shapDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_groups = find_correlation_groups(shapDF, corr_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapDF.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
