{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load required Liabraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import shap\n",
    "from scorecardutils.feature_selection import (shap_feature_selection,\n",
    "                                              find_correlation_groups,\n",
    "                                              select_best_features_from_corr_groups,\n",
    "                                              vsi_check)\n",
    "from scorecardutils.BivariatePlot import enhanced_bivariate_plot,transform_and_plot_oot_bivariate_data\n",
    "\n",
    "#from feature_engine.selection import SmartCorrelatedSelection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv('../data/credit_risk_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure each feature has correct data types --float,int,catgeory\n",
    "for col in dev_data.select_dtypes(include='object').columns:\n",
    "    dev_data[col] = dev_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define target and features names\n",
    "target = 'default'\n",
    "features = dev_data.drop(columns=[target]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",       # For binary classification\n",
    "    \"eval_metric\": \"auc\",                 # auc\n",
    "    \"learning_rate\": 0.05,                 # Smaller means more trees; safer\n",
    "    \"max_depth\": 6,                       # Controls model complexity\n",
    "    \"subsample\": 0.8,                     # Fraction of samples per tree\n",
    "    \"colsample_bytree\": 0.8,              # Fraction of features per tree\n",
    "    \"lambda\": 1,                          # L2 regularization\n",
    "    \"alpha\": 0,                           # L1 regularization\n",
    "    \"n_estimators\": 100,                  # Total trees\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features,importance_df,_ =shap_feature_selection(train_data=dev_data,feature_names=features,target_name='default',verbose=True,\n",
    "                                                        test_size=0.3,random_state=42,use_train_for_shap=False,\n",
    "                                                        model_params=xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,importance_df,shapDF =shap_feature_selection(train_data=dev_data,feature_names=selected_features,target_name='default',verbose=False,\n",
    "                                                        split_data=False,random_state=42,\n",
    "                                                        model_params=xgb_params,create_shap_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_groups = find_correlation_groups(shapDF, corr_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,selected_features_corr = select_best_features_from_corr_groups(correlated_groups, feature_importance_df=importance_df,\n",
    "                                      feature_importance_col='SHAP_Importance',\n",
    "                                      feature_name_col='Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Binning and IV ,Stability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optbinning import BinningProcess\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndata= pd.read_csv('../data/synthetic_binary_classification_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= syndata.drop(columns=[target])\n",
    "y= syndata[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vars= X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract categorical and numerical columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "numerical_columns = X_train.select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Any Special Codes to treat them in separate bucket\n",
    "e.g \n",
    "special_codes = [-999,-1,-2]\n",
    "or if need to treat separate codes with different values in different special buckets\n",
    "special_codes = {'special_1': -9, \"special_2\": -8, \"special_3\": -7}\n",
    "\"\"\"\n",
    "\n",
    "special_codes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Dictionary with optimal binning fitting options for specific variables. we can update it as per our customization\n",
    "for specific attributes.\n",
    "split_digits: If split_digits is set to 0, the split points are integers otherwise the split points are rounded \n",
    "to the number of digits specified by split_digits.\n",
    "user_splits: If user_splits is set, the splits are fixed to the values specified by user_splits.\n",
    "user_splits_fixed: If user_splits_fixed is set to True, the splits are fixed to the values specified by user_splits.\n",
    "monotonic_trend: If monotonic_trend is set to 'ascending', the bad rate should be non-decreasing.\n",
    "cat_cutoff:Generate bin others with categories in which the fraction of occurrences is below the cat_cutoff value. \n",
    "i.e If cat_cutoff is set to 0.05, the bin will be generated with categories in which the fraction of occurrences is below the cat_cutoff value.\n",
    "\n",
    "Initially can be passed as None. binning_fit_params=None\n",
    "\n",
    "binning_fit_params = {\n",
    "    \"dti\": {\"monotonic_trend\": \"ascending\",\"split_digits\":2 ,\n",
    "            \"user_splits\": [ 8.89, 10.91, 14.68, 16.03,18.23, 20.8 , 22.11, 28.37],\n",
    "           # \"user_splits_fixed\" :[True,True] \n",
    "           }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "binning_fit_params = {\n",
    "\n",
    "    \"loyalty_score\":{\"split_digits\":2,\n",
    "        \"user_splits\": [    -3.07959914, -2.46296906, -1.8946799 , -1.6182403 , -1 ,\n",
    "                            0,  0.37816253,  0.77244589,  1.2557528 ,\n",
    "                            1.77894938],\n",
    "        \"user_splits_fixed\": [False,False,False,False,True,True,False,False,False,False]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By default:optbinning assigns WoE = 0 and event rate = overall mean to special values (like missing or outliers), unless you override it.\n",
    "Update Strategy for Special Values:\n",
    "Strategy A: Neutralize Special Values i.e.Assign WoE = 0 (effectively no contribution to the score).\n",
    "Strategy B: Assign Empirical WoE for Special Values i.e.Let the special values’ WoE be calculated based on their actual event rate in the data.This is often used when special codes have meaningful predictive power\n",
    "Strategy C: Merge Special Value into Closest Bin. f a special code behaves like a particular bin (e.g., 999 behaves like bin [30–40]), assign its WoE manually to match that bin’s WoE\n",
    "\n",
    "binning_transform_params ={\n",
    "   'revol_util':{'metric_special':'empirical'},\n",
    "    'dti':{'metric_special': -0.306345},\n",
    "    'inq_last_6mths':{'metric_special':'empirical'}\n",
    "}\n",
    "\n",
    "or \n",
    "binning_transform_params = None\n",
    "\"\"\"\n",
    "binning_transform_params = {\n",
    "    'age':{'metric_missing':'empirical','metric_special':'empirical'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "List of variables to be fixed. \n",
    "The binning process will retain these variables if the selection criteria is not satisfied.\n",
    "\"\"\"\n",
    "fixed_variables=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the selection criteria for the binning process\n",
    "selection_criteria = {\n",
    "    \"iv\": {\"min\": 0.01, \"max\": 0.5,\"strategy\": \"highest\", \"top\": 50},\n",
    "    \"quality_score\": {\"min\": 0.01}\n",
    "}\n",
    "\n",
    "or \n",
    "selection_criteria = None\n",
    "\"\"\"\n",
    "selection_criteria = {\n",
    "    \"iv\": {\"min\": 0.01,\"strategy\": \"highest\", \"top\": 50}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning_process = BinningProcess(variable_names=model_vars, special_codes=special_codes,\n",
    "                                 categorical_variables=categorical_columns,\n",
    "                                 selection_criteria=selection_criteria,\n",
    "                                 binning_fit_params=binning_fit_params,\n",
    "                                 binning_transform_params=binning_transform_params,\n",
    "                                fixed_variables=fixed_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the binning process    \n",
    "binning_process.fit(X=X_train[model_vars], y=y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save all the variables passed to binning process and their IVs for manual screening\n",
    "iv_tab=binning_process.summary().sort_values(by='iv',ascending=False)\n",
    "#iv_tab.to_excel('iv_tab.xlsx', index=False)\n",
    "iv_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_selected_variables = iv_tab[iv_tab['iv']>0.02]['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_selected_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To see any specific variable binning table\n",
    "optb = binning_process.get_binned_variable('customer_group')\n",
    "df = optb.binning_table.build()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CSI summary\n",
    "csi_summ = vsi_check(\n",
    "    X_oot=X_test, \n",
    "    X_train=X_train,\n",
    "    binning_process=binning_process,\n",
    "    style='summary',  # or 'detailed' for bin-level information\n",
    "    psi_min_bin_size=0.01,\n",
    "    max_workers=4  # Adjust based on your CPU cores\n",
    ")\n",
    "\n",
    "## CSI detailed Summary\n",
    "csi_det = vsi_check(\n",
    "    X_oot=X_test, \n",
    "    X_train=X_train,\n",
    "    binning_process=binning_process,\n",
    "    style='detailed',  # or 'detailed' for bin-level information\n",
    "    psi_min_bin_size=0.01,\n",
    "    max_workers=4  # Adjust based on your CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save csi in excel\n",
    "with pd.ExcelWriter('csi.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "    csi_summ.to_excel(writer, sheet_name='summary', index=False)\n",
    "    csi_det.to_excel(writer, sheet_name='detail', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stable variables (PSI < threshold)\n",
    "stable_variables = [str(var) for var in csi_summ[csi_summ['CSI'] < 10]['Variable'].tolist()]\n",
    "    \n",
    "# Filter unstable variables (PSI >= threshold)\n",
    "unstable_variables = [str(var) for var in csi_summ[csi_summ['CSI'] >= 10]['Variable'].tolist()]\n",
    "print(\"Unstable Variables:\", unstable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_ivs = [feature for feature in iv_selected_variables if feature not in unstable_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Bivariate plot on both Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optb = binning_process.get_binned_variable('age')\n",
    "\n",
    "# Get DataFrame from binning table\n",
    "df = optb.binning_table.build()\n",
    "df = df.drop(columns='JS')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl import Workbook\n",
    "from typing import List, Optional, Union, Dict, Any, Tuple\n",
    "\n",
    "def unified_bivariate_analysis(\n",
    "    binning_process,\n",
    "    filename: str = 'bivariate_analysis',\n",
    "    metric: str = 'event_rate',\n",
    "    variables: Optional[List[str]] = None,\n",
    "    figsize: tuple = (12, 6),\n",
    "    dpi: int = 100,\n",
    "    style: str = 'whitegrid',\n",
    "    oot_data: Optional[pd.DataFrame] = None,\n",
    "    target_column: Optional[str] = None,\n",
    "    compare_data: bool = True,\n",
    "    show_bar_values: bool = False,\n",
    "    verbose: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Unified function to create bivariate analysis plots for binned data with options for:\n",
    "    1. Training data only\n",
    "    2. OOT data only (based on training binning)\n",
    "    3. Comparison between training and OOT data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binning_process : OptimalBinning process object\n",
    "        The binning process containing the variables to plot\n",
    "    filename : str, default='bivariate_analysis'\n",
    "        Base name for the output Excel file\n",
    "    metric : str, default='event_rate'\n",
    "        Metric to plot. Options: 'event_rate', 'woe'\n",
    "    variables : list of str, optional\n",
    "        List of specific variable names to process. If None, all variables are processed.\n",
    "    figsize : tuple, default=(12, 6)\n",
    "        Figure size for plots in inches (width, height)\n",
    "    dpi : int, default=100\n",
    "        Resolution of saved images\n",
    "    style : str, default='whitegrid'\n",
    "        Seaborn style for plots\n",
    "    oot_data : pandas DataFrame, optional\n",
    "        The OOT dataset to transform and analyze. If None, only training data is plotted.\n",
    "    target_column : str, optional\n",
    "        Name of the target/outcome column in the OOT data. Required if oot_data is provided.\n",
    "    compare_data : bool, default=True\n",
    "        Whether to plot training data metrics alongside OOT data metrics.\n",
    "        Only relevant when oot_data is provided.\n",
    "    show_bar_values : bool, default=False\n",
    "        Whether to show Count (%) values on top of the bars\n",
    "    verbose : bool, default=False\n",
    "        Whether to print detailed information during processing\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if oot_data is not None and target_column is None:\n",
    "        raise ValueError(\"target_column must be specified when oot_data is provided\")\n",
    "    \n",
    "    # Set mode based on inputs\n",
    "    if oot_data is None:\n",
    "        mode = \"train_only\"\n",
    "    else:\n",
    "        mode = \"oot_comparison\" if compare_data else \"oot_only\"\n",
    "        \n",
    "    # Validate metric parameter\n",
    "    metric = metric.lower()\n",
    "    if metric not in ['event_rate', 'woe']:\n",
    "        raise ValueError(\"metric must be either 'event_rate' or 'woe'\")\n",
    "    \n",
    "    # Get all available variables from binning process\n",
    "    try:\n",
    "        # Try to get variable names directly\n",
    "        all_variables = binning_process.variable_names\n",
    "    except AttributeError:\n",
    "        # Fall back to get_support if variable_names is not available\n",
    "        try:\n",
    "            all_variables = binning_process.get_support(names=True)\n",
    "        except AttributeError:\n",
    "            raise ValueError(\"Unable to extract variable names from binning_process\")\n",
    "    \n",
    "    # Filter variables if specified\n",
    "    if variables is not None:\n",
    "        # Check if all specified variables exist in the binning process\n",
    "        invalid_vars = set(variables) - set(all_variables)\n",
    "        if invalid_vars:\n",
    "            raise ValueError(f\"Variables not found in binning process: {', '.join(invalid_vars)}\")\n",
    "        selected_vars = variables\n",
    "    else:\n",
    "        selected_vars = all_variables\n",
    "    \n",
    "    # Track if we've created at least one valid sheet for Excel\n",
    "    valid_sheets_created = False\n",
    "    \n",
    "    # Set up the visualization style\n",
    "    plt.style.use('default')  # Clean matplotlib style\n",
    "    \n",
    "    # Create a temporary directory for images if it doesn't exist\n",
    "    if not os.path.exists('temp_plots'):\n",
    "        os.makedirs('temp_plots')\n",
    "    \n",
    "    # Dictionary to store plot paths and binning tables\n",
    "    plot_paths = {}\n",
    "    oot_binning_tables = {}\n",
    "    \n",
    "    # Determine column names based on metric\n",
    "    metric_column = 'Event rate' if metric == 'event_rate' else 'WoE'\n",
    "    y_axis_label = 'Event Rate' if metric == 'event_rate' else 'Weight of Evidence (WoE)'\n",
    "    \n",
    "    # Create Excel writer\n",
    "    with pd.ExcelWriter(f'{filename}.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "        # Process each selected variable\n",
    "        for var in selected_vars:\n",
    "            try:\n",
    "                # Sanitize variable name for Excel sheet name (remove invalid characters)\n",
    "                sheet_name = str(var)\n",
    "                for char in ['[', ']', ':', '*', '?', '/', '\\\\']:\n",
    "                    sheet_name = sheet_name.replace(char, '_')\n",
    "                # Ensure sheet name is not longer than 31 characters (Excel limit)\n",
    "                if len(sheet_name) > 31:\n",
    "                    sheet_name = sheet_name[:31]\n",
    "                    \n",
    "                # Get the binned variable from the trained process\n",
    "                try:\n",
    "                    optb = binning_process.get_binned_variable(var)\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Error getting binned variable {var}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get original binning table from training data\n",
    "                train_df = optb.binning_table.build()\n",
    "                if 'JS' in train_df.columns:\n",
    "                    train_df = train_df.drop(columns='JS')\n",
    "                \n",
    "                # Calculate IV if it doesn't exist\n",
    "                if 'IV' not in train_df.columns:\n",
    "                    # Calculate IV for each bin\n",
    "                    train_df['IV'] = train_df.apply(\n",
    "                        lambda row: (row['Non-event (%)'] - row['Event (%)']) * row['WoE'] \n",
    "                        if row.name != 'Totals' else np.NaN, \n",
    "                        axis=1\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate total IV (sum of all bin IVs)\n",
    "                    total_iv = train_df['IV'].sum()\n",
    "                    \n",
    "                    # Add total IV to the Totals row if it exists\n",
    "                    if 'Totals' in train_df.index:\n",
    "                        train_df.loc['Totals', 'IV'] = total_iv\n",
    "                \n",
    "                # Keep the original DataFrame for Excel output including Totals row\n",
    "                train_df_for_excel = train_df.reset_index()\n",
    "                \n",
    "                # Add Dataset column to training data\n",
    "                train_df_for_excel.insert(0, 'Dataset', 'Training')\n",
    "                \n",
    "                # Extract the Totals row and prepare for plotting\n",
    "                train_totals_row = None\n",
    "                train_df_no_totals = train_df\n",
    "                if 'Totals' in train_df.index:\n",
    "                    train_totals_row = train_df.loc['Totals']\n",
    "                    train_df_no_totals = train_df.drop('Totals')\n",
    "                \n",
    "                # Reset index for plotting\n",
    "                train_df_reset = train_df_no_totals.reset_index()\n",
    "                \n",
    "                # The actual bin column is 'Bin', but if it doesn't exist, use the first column as fallback\n",
    "                bin_col_name = 'Bin' if 'Bin' in train_df_reset.columns else train_df_reset.columns[0]\n",
    "                \n",
    "                # Process OOT data if provided\n",
    "                oot_df = None\n",
    "                oot_df_with_totals = None\n",
    "                total_event_rate_oot = None\n",
    "                total_iv_oot = None\n",
    "                \n",
    "                if oot_data is not None:\n",
    "                    # Transform the OOT data for this variable using the trained binning\n",
    "                    try:\n",
    "                        X_var_oot = oot_data[var].values\n",
    "                        y_oot = oot_data[target_column].values\n",
    "                    except KeyError as e:\n",
    "                        if verbose:\n",
    "                            print(f\"Variable {var} or target column {target_column} not found in OOT data: {str(e)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Use the transform method with \"bins\" metric to get bin assignments\n",
    "                    try:\n",
    "                        oot_bins = optb.transform(X_var_oot, metric=\"bins\")\n",
    "                    except Exception as e:\n",
    "                        if verbose:\n",
    "                            print(f\"Error transforming OOT data for variable {var}: {str(e)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Create a dictionary to store counts for each bin - convert to strings to make hashable\n",
    "                    bin_n_event = {str(bin_name): 0 for bin_name in train_df_reset[bin_col_name]}\n",
    "                    bin_n_nonevent = {str(bin_name): 0 for bin_name in train_df_reset[bin_col_name]}\n",
    "                    \n",
    "                    # Count events and non-events for each bin in OOT data\n",
    "                    for i, bin_label in enumerate(oot_bins):\n",
    "                        bin_label_str = str(bin_label)\n",
    "                        if bin_label_str in bin_n_event:\n",
    "                            if y_oot[i] == 1:\n",
    "                                bin_n_event[bin_label_str] += 1\n",
    "                            else:\n",
    "                                bin_n_nonevent[bin_label_str] += 1\n",
    "                    \n",
    "                    # Calculate totals for percentages\n",
    "                    total_events_oot = sum(bin_n_event.values())\n",
    "                    total_nonevents_oot = sum(bin_n_nonevent.values())\n",
    "                    total_records_oot = total_events_oot + total_nonevents_oot\n",
    "                    \n",
    "                    # Create a new DataFrame for OOT data metrics\n",
    "                    oot_stats = []\n",
    "                    total_iv_oot = 0\n",
    "                    for bin_name in train_df_reset[bin_col_name]:\n",
    "                        bin_name_str = str(bin_name)\n",
    "                        n_event = bin_n_event[bin_name_str]\n",
    "                        n_nonevent = bin_n_nonevent[bin_name_str]\n",
    "                        n_records = n_event + n_nonevent\n",
    "                        \n",
    "                        # Calculate event rate for this bin\n",
    "                        event_rate = n_event / n_records if n_records > 0 else 0\n",
    "                        \n",
    "                        # Calculate event and non-event percentages\n",
    "                        event_pct = n_event / total_events_oot if total_events_oot > 0 else 0\n",
    "                        nonevent_pct = n_nonevent / total_nonevents_oot if total_nonevents_oot > 0 else 0\n",
    "                        \n",
    "                        # Calculate WoE similar to the binning_table.build() method\n",
    "                        if n_event > 0 and n_nonevent > 0 and total_events_oot > 0 and total_nonevents_oot > 0:\n",
    "                            p_event = n_event / total_events_oot\n",
    "                            p_nonevent = n_nonevent / total_nonevents_oot\n",
    "                            woe = np.log(p_nonevent / p_event)\n",
    "                        else:\n",
    "                            woe = 0  # Default value\n",
    "                        \n",
    "                        # Calculate IV for this bin\n",
    "                        iv = (nonevent_pct - event_pct) * woe\n",
    "                        total_iv_oot += iv\n",
    "                        \n",
    "                        oot_stats.append({\n",
    "                            bin_col_name: bin_name,  # Keep as original type for consistency\n",
    "                            'Count': n_records,\n",
    "                            'Count (%)': n_records / total_records_oot if total_records_oot > 0 else 0,\n",
    "                            'Non-event': n_nonevent,\n",
    "                            'Non-event (%)': nonevent_pct,\n",
    "                            'Event': n_event,\n",
    "                            'Event (%)': event_pct,\n",
    "                            'Event rate': event_rate,\n",
    "                            'WoE': woe,\n",
    "                            'IV': iv\n",
    "                        })\n",
    "                    \n",
    "                    # Create DataFrame from collected statistics\n",
    "                    oot_df = pd.DataFrame(oot_stats)\n",
    "                    \n",
    "                    # Calculate total event rate for OOT\n",
    "                    total_event_rate_oot = total_events_oot / total_records_oot if total_records_oot > 0 else 0\n",
    "                    \n",
    "                    # Add totals row for OOT\n",
    "                    totals_row_oot = {\n",
    "                        bin_col_name: 'Totals',\n",
    "                        'Count': total_records_oot,\n",
    "                        'Count (%)': 1.0,\n",
    "                        'Non-event': total_nonevents_oot,\n",
    "                        'Non-event (%)': 1.0,\n",
    "                        'Event': total_events_oot,\n",
    "                        'Event (%)': 1.0,\n",
    "                        'Event rate': total_event_rate_oot,\n",
    "                        'WoE': '',  # WoE doesn't make sense for totals\n",
    "                        'IV': total_iv_oot\n",
    "                    }\n",
    "                    \n",
    "                    # Add totals row to OOT data\n",
    "                    oot_df_with_totals = pd.concat([oot_df, pd.DataFrame([totals_row_oot])], ignore_index=True)\n",
    "                    \n",
    "                    # Add Dataset column to OOT data\n",
    "                    oot_df_with_totals.insert(0, 'Dataset', 'OOT')\n",
    "                    \n",
    "                    # Store OOT binning table\n",
    "                    oot_binning_tables[var] = oot_df_with_totals\n",
    "                \n",
    "                # Calculate missing Event (%) and Non-event (%) for Training data if they don't exist\n",
    "                if 'Event (%)' not in train_df_for_excel.columns or train_df_for_excel['Event (%)'].isna().all():\n",
    "                    # Check if 'Totals' row exists\n",
    "                    totals_mask = train_df_for_excel[bin_col_name] == 'Totals'\n",
    "                    \n",
    "                    if totals_mask.any():\n",
    "                        # Get total events from Totals row\n",
    "                        total_events_train = train_df_for_excel.loc[totals_mask, 'Event'].values[0]\n",
    "                        total_nonevents_train = train_df_for_excel.loc[totals_mask, 'Non-event'].values[0]\n",
    "                        \n",
    "                        # Calculate percentages only for non-Totals rows\n",
    "                        non_totals_mask = ~totals_mask\n",
    "                        train_df_for_excel.loc[non_totals_mask, 'Event (%)'] = train_df_for_excel.loc[non_totals_mask, 'Event'] / total_events_train\n",
    "                        train_df_for_excel.loc[non_totals_mask, 'Non-event (%)'] = train_df_for_excel.loc[non_totals_mask, 'Non-event'] / total_nonevents_train\n",
    "                        \n",
    "                        # Set Totals row percentages to 1.0\n",
    "                        train_df_for_excel.loc[totals_mask, 'Event (%)'] = 1.0\n",
    "                        train_df_for_excel.loc[totals_mask, 'Non-event (%)'] = 1.0\n",
    "                    else:\n",
    "                        # Calculate totals directly from data (excluding totals row if it exists)\n",
    "                        total_events_train = train_df_for_excel['Event'].sum()\n",
    "                        total_nonevents_train = train_df_for_excel['Non-event'].sum()\n",
    "                        \n",
    "                        # Add a Totals row\n",
    "                        totals_row = {\n",
    "                            'Dataset': 'Training',\n",
    "                            bin_col_name: 'Totals',\n",
    "                            'Count': total_events_train + total_nonevents_train,\n",
    "                            'Count (%)': 1.0,\n",
    "                            'Non-event': total_nonevents_train,\n",
    "                            'Non-event (%)': 1.0,\n",
    "                            'Event': total_events_train,\n",
    "                            'Event (%)': 1.0,\n",
    "                            'Event rate': total_events_train / (total_events_train + total_nonevents_train) if (total_events_train + total_nonevents_train) > 0 else 0,\n",
    "                            'WoE': '',\n",
    "                            'IV': train_df_for_excel['IV'].sum() if 'IV' in train_df_for_excel.columns else 0\n",
    "                        }\n",
    "                        train_df_for_excel = pd.concat([train_df_for_excel, pd.DataFrame([totals_row])], ignore_index=True)\n",
    "                        \n",
    "                        # Calculate percentages for all non-Totals rows\n",
    "                        train_df_for_excel.loc[train_df_for_excel[bin_col_name] != 'Totals', 'Event (%)'] = (\n",
    "                            train_df_for_excel.loc[train_df_for_excel[bin_col_name] != 'Totals', 'Event'] / total_events_train\n",
    "                        )\n",
    "                        train_df_for_excel.loc[train_df_for_excel[bin_col_name] != 'Totals', 'Non-event (%)'] = (\n",
    "                            train_df_for_excel.loc[train_df_for_excel[bin_col_name] != 'Totals', 'Non-event'] / total_nonevents_train\n",
    "                        )\n",
    "                \n",
    "                # Check if we have double Totals rows in training data and remove duplicates if necessary\n",
    "                totals_rows = train_df_for_excel[train_df_for_excel[bin_col_name] == 'Totals']\n",
    "                if len(totals_rows) > 1:\n",
    "                    # Keep only the first Totals row\n",
    "                    train_df_for_excel = pd.concat([\n",
    "                        train_df_for_excel[train_df_for_excel[bin_col_name] != 'Totals'],\n",
    "                        totals_rows.iloc[[0]]\n",
    "                    ]).reset_index(drop=True)\n",
    "                \n",
    "                # Determine which data to write to Excel based on mode\n",
    "                if mode == \"train_only\":\n",
    "                    # Add variable name header\n",
    "                    var_header = pd.DataFrame([{col: '' for col in train_df_for_excel.columns}])\n",
    "                    var_header['Dataset'] = f'{var}'\n",
    "                    \n",
    "                    # Combine header and data\n",
    "                    df_for_excel = pd.concat([var_header, train_df_for_excel], ignore_index=True)\n",
    "                    \n",
    "                elif mode == \"oot_only\":\n",
    "                    # Add variable name header\n",
    "                    var_header = pd.DataFrame([{col: '' for col in oot_df_with_totals.columns}])\n",
    "                    var_header['Dataset'] = f'{var}'\n",
    "                    \n",
    "                    # Combine header and data\n",
    "                    df_for_excel = pd.concat([var_header, oot_df_with_totals], ignore_index=True)\n",
    "                    \n",
    "                else:  # oot_comparison - write both with headers and gap\n",
    "                    # Ensure both dataframes have the same columns in the same order\n",
    "                    all_columns = list(set(train_df_for_excel.columns) | set(oot_df_with_totals.columns))\n",
    "                    \n",
    "                    # Move 'Dataset' column to the beginning if present\n",
    "                    if 'Dataset' in all_columns:\n",
    "                        all_columns.remove('Dataset')\n",
    "                        all_columns = ['Dataset'] + all_columns\n",
    "                    \n",
    "                    # Ensure both dataframes have all columns\n",
    "                    for col in all_columns:\n",
    "                        if col not in train_df_for_excel.columns:\n",
    "                            train_df_for_excel[col] = ''\n",
    "                        if col not in oot_df_with_totals.columns:\n",
    "                            oot_df_with_totals[col] = ''\n",
    "                    \n",
    "                    # Reorder columns to match\n",
    "                    train_df_for_excel = train_df_for_excel[all_columns]\n",
    "                    oot_df_with_totals = oot_df_with_totals[all_columns]\n",
    "                    \n",
    "                    # Add variable name header\n",
    "                    var_header = pd.DataFrame([{col: '' for col in all_columns}])\n",
    "                    var_header['Dataset'] = f'{var}'\n",
    "                    \n",
    "                    # Add training data header\n",
    "                    train_header = pd.DataFrame([{col: '' for col in all_columns}])\n",
    "                    train_header['Dataset'] = 'Training Data'\n",
    "                    \n",
    "                    # Create a blank row for spacing\n",
    "                    blank_row = pd.DataFrame([{col: '' for col in all_columns}])\n",
    "                    \n",
    "                    # Add OOT data header\n",
    "                    oot_header = pd.DataFrame([{col: '' for col in all_columns}])\n",
    "                    oot_header['Dataset'] = 'OOT Data'\n",
    "                    \n",
    "                    # Combine all parts: variable name, training header, training data, blank row, OOT header, OOT data\n",
    "                    df_for_excel = pd.concat([\n",
    "                        var_header,\n",
    "                        train_header, \n",
    "                        train_df_for_excel, \n",
    "                        blank_row, \n",
    "                        oot_header, \n",
    "                        oot_df_with_totals\n",
    "                    ], ignore_index=True)\n",
    "                \n",
    "                # Reorder columns to the desired structure\n",
    "                desired_column_order = [\n",
    "                    'Dataset', \n",
    "                    bin_col_name, \n",
    "                    'Count', \n",
    "                    'Count (%)', \n",
    "                    'Non-event', \n",
    "                    'Non-event (%)', \n",
    "                    'Event', \n",
    "                    'Event (%)', \n",
    "                    'Event rate', \n",
    "                    'WoE', \n",
    "                    'IV'\n",
    "                ]\n",
    "                \n",
    "                # Add any missing columns that might be in df_for_excel but not in our desired order\n",
    "                for col in df_for_excel.columns:\n",
    "                    if col not in desired_column_order and col != 'index':\n",
    "                        desired_column_order.append(col)\n",
    "                \n",
    "                # Reorder columns (only for columns that exist in the DataFrame)\n",
    "                existing_columns = [col for col in desired_column_order if col in df_for_excel.columns]\n",
    "                df_for_excel = df_for_excel[existing_columns]\n",
    "                \n",
    "                # Write to Excel\n",
    "                df_for_excel.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                valid_sheets_created = True\n",
    "                \n",
    "                # Create plot image path\n",
    "                plot_image_path = f'temp_plots/variable_{var}_{metric}_{mode}.png'\n",
    "                \n",
    "                # Create the appropriate plot based on mode\n",
    "                fig, ax1 = plt.subplots(figsize=figsize)\n",
    "                \n",
    "                if mode == \"train_only\":\n",
    "                    # Plot training data only\n",
    "                    plot_single_dataset(\n",
    "                        ax1, train_df_reset, bin_col_name, metric_column, \n",
    "                        y_axis_label, train_totals_row, \"Training\", line_color='darkgoldenrod', \n",
    "                        bar_color='#0a3f7d', show_bar_values=show_bar_values\n",
    "                    )\n",
    "                    plt.title(f'{var}: {y_axis_label} by Bin (Training Data)', fontsize=12)\n",
    "                    \n",
    "                elif mode == \"oot_only\":\n",
    "                    # Plot OOT data only\n",
    "                    plot_single_dataset(\n",
    "                        ax1, oot_df, bin_col_name, metric_column, \n",
    "                        y_axis_label, pd.Series({'Event rate': total_event_rate_oot, 'WoE': '', 'IV': total_iv_oot}), \n",
    "                        \"OOT\", line_color='darkgoldenrod', bar_color='#0a3f7d', show_bar_values=show_bar_values\n",
    "                    )\n",
    "                    plt.title(f'{var}: {y_axis_label} by Bin (OOT Data)', fontsize=12)\n",
    "                    \n",
    "                else:  # oot_comparison\n",
    "                    # Plot comparison between training and OOT data\n",
    "                    plot_comparison(\n",
    "                        ax1, train_df_reset, oot_df, bin_col_name, metric_column, \n",
    "                        y_axis_label, train_totals_row, total_event_rate_oot, metric, show_bar_values=show_bar_values\n",
    "                    )\n",
    "                    plt.title(f'{var}: {y_axis_label} Comparison (Training vs OOT)', fontsize=12)\n",
    "                \n",
    "                # Adjust layout and save\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(plot_image_path, dpi=dpi, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Store path for cleanup\n",
    "                plot_paths[var] = plot_image_path\n",
    "                \n",
    "                # Insert plot image into Excel\n",
    "                try:\n",
    "                    img = Image(plot_image_path)\n",
    "                    \n",
    "                    # Start plot after the data table with some margin\n",
    "                    row_position = len(df_for_excel) + 4\n",
    "                    img.anchor = f'A{row_position}'\n",
    "                    \n",
    "                    writer.sheets[sheet_name].add_image(img)\n",
    "                    \n",
    "                    # Adjust column widths\n",
    "                    for idx, col in enumerate(df_for_excel.columns):\n",
    "                        column_width = max(len(str(col)), df_for_excel[col].astype(str).map(len).max())\n",
    "                        writer.sheets[sheet_name].column_dimensions[chr(65 + idx)].width = column_width + 2\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Error adding image to Excel for variable {var}: {str(e)}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Error processing variable {var}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Add a dummy sheet if no valid sheets were created to avoid Excel error\n",
    "        if not valid_sheets_created:\n",
    "            dummy_df = pd.DataFrame({'Message': ['No valid variables found for analysis']})\n",
    "            dummy_df.to_excel(writer, sheet_name='Info', index=False)\n",
    "            if verbose:\n",
    "                print(\"No valid sheets were created. Adding a dummy sheet.\")\n",
    "    \n",
    "    # Clean up temporary image files\n",
    "    for path in plot_paths.values():\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "    \n",
    "    # Remove temp directory if empty\n",
    "    if os.path.exists('temp_plots') and not os.listdir('temp_plots'):\n",
    "        os.rmdir('temp_plots')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Bivariate analysis completed! Results saved to {filename}.xlsx\")\n",
    "    \n",
    "    # Return None instead of the oot_binning_tables to avoid printing\n",
    "    return None\n",
    "def plot_single_dataset(\n",
    "    ax1, df, bin_col_name, metric_column, y_axis_label, \n",
    "    totals_row, dataset_name, line_color='darkgoldenrod', bar_color='#0a3f7d',\n",
    "    show_bar_values=False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Helper function to plot a single dataset (either training or OOT)\n",
    "    \"\"\"\n",
    "    # Ensure bin column is string type\n",
    "    df[bin_col_name] = df[bin_col_name].astype(str)\n",
    "    \n",
    "    # Identify regular, special, and missing bins\n",
    "    regular_bins = df[~df[bin_col_name].str.contains('Special|Missing', regex=True, na=False)]\n",
    "    special_bin = df[df[bin_col_name].str.contains('Special', regex=False, na=False)]\n",
    "    missing_bin = df[df[bin_col_name].str.contains('Missing', regex=False, na=False)]\n",
    "    \n",
    "    # Create indices for x-axis\n",
    "    x_indices = np.arange(len(df))\n",
    "    \n",
    "    # Format bin labels\n",
    "    bin_labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        bin_name = row[bin_col_name]\n",
    "        if 'Special' in bin_name or 'Missing' in bin_name:\n",
    "            bin_labels.append(bin_name)\n",
    "        else:\n",
    "            shortened_name = str(bin_name)\n",
    "            if len(shortened_name) > 15:\n",
    "                shortened_name = shortened_name[:12] + \"...\"\n",
    "            bin_labels.append(shortened_name)\n",
    "    \n",
    "    # Plot Count (%) as bars\n",
    "    bars = ax1.bar(x_indices, df['Count (%)'] * 100, color=bar_color, alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of bars if requested\n",
    "    if show_bar_values:\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(\n",
    "                bar.get_x() + bar.get_width()/2.,\n",
    "                height + 0.5,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=8, color='black'\n",
    "            )\n",
    "    \n",
    "    ax1.set_xticks(x_indices)\n",
    "    ax1.set_xticklabels(bin_labels, rotation=45, ha='right', fontsize=9)\n",
    "    ax1.set_xlabel('Bins', fontsize=10)\n",
    "    ax1.set_ylabel('Count (%)', color='blue', fontsize=10)\n",
    "    ax1.tick_params(axis='y', labelcolor='blue', labelsize=9)\n",
    "    \n",
    "    # Create second y-axis for Event Rate or WoE\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot the metric line for regular bins only\n",
    "    if not regular_bins.empty:\n",
    "        regular_mask = ~df[bin_col_name].str.contains('Special|Missing', regex=True, na=False)\n",
    "        regular_indices = np.where(regular_mask)[0]\n",
    "        \n",
    "        if len(regular_indices) > 0:\n",
    "            ax2.plot(regular_indices, regular_bins[metric_column], \n",
    "                     marker='o', color=line_color, linewidth=2, label=f'{dataset_name} {y_axis_label}')\n",
    "            \n",
    "            # Add value annotations\n",
    "            for idx, val in zip(regular_indices, regular_bins[metric_column]):\n",
    "                ax2.annotate(f'{val:.3f}', \n",
    "                             xy=(idx, val), \n",
    "                             xytext=(0, 5),\n",
    "                             textcoords='offset points',\n",
    "                             ha='center', \n",
    "                             fontsize=7)\n",
    "    \n",
    "    # Plot Special bin point (if exists)\n",
    "    if not special_bin.empty:\n",
    "        special_indices = df[df[bin_col_name].str.contains('Special', regex=False, na=False)].index\n",
    "        for idx in special_indices:\n",
    "            special_val = df.loc[idx, metric_column]\n",
    "            ax2.plot(idx, special_val,\n",
    "                     marker='s', color='red', markersize=8, linestyle='None', \n",
    "                     label='Special' if idx == special_indices[0] else \"\")\n",
    "            ax2.annotate(f'{special_val:.3f}', \n",
    "                         xy=(idx, special_val), \n",
    "                         xytext=(0, 5),\n",
    "                         textcoords='offset points',\n",
    "                         ha='center', \n",
    "                         fontsize=7)\n",
    "    \n",
    "    # Plot Missing bin point (if exists)\n",
    "    if not missing_bin.empty:\n",
    "        missing_indices = df[df[bin_col_name].str.contains('Missing', regex=False, na=False)].index\n",
    "        for idx in missing_indices:\n",
    "            missing_val = df.loc[idx, metric_column]\n",
    "            ax2.plot(idx, missing_val,\n",
    "                     marker='D', color='purple', markersize=8, linestyle='None', \n",
    "                     label='Missing' if idx == missing_indices[0] else \"\")\n",
    "            ax2.annotate(f'{missing_val:.3f}', \n",
    "                         xy=(idx, missing_val), \n",
    "                         xytext=(0, 5),\n",
    "                         textcoords='offset points',\n",
    "                         ha='center', \n",
    "                         fontsize=7)\n",
    "    \n",
    "    # Add horizontal line for Totals if available\n",
    "    if totals_row is not None:\n",
    "        total_metric_value = totals_row[metric_column]\n",
    "        \n",
    "        # Handle case where value might be a blank string\n",
    "        if isinstance(total_metric_value, str) and total_metric_value.strip() == '':\n",
    "            total_metric_value = 0.0\n",
    "        else:\n",
    "            try:\n",
    "                total_metric_value = float(total_metric_value)\n",
    "            except (ValueError, TypeError):\n",
    "                total_metric_value = 0.0\n",
    "        \n",
    "        ax2.axhline(y=total_metric_value, color=line_color, linestyle='--', \n",
    "                   alpha=0.7, label=f'{dataset_name} Total: {total_metric_value:.4f}')\n",
    "        \n",
    "        # Add IV information to legend if available\n",
    "        if 'IV' in totals_row and totals_row['IV'] not in (None, '', np.nan):\n",
    "            try:\n",
    "                iv_value = float(totals_row['IV'])\n",
    "                ax2.plot([], [], ' ', label=f'IV: {iv_value:.4f}')\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    # Set y-axis label\n",
    "    ax2.set_ylabel(y_axis_label, color=line_color, fontsize=11)\n",
    "    ax2.tick_params(axis='y', labelcolor=line_color)\n",
    "    \n",
    "    # Add legend with unique entries\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax2.legend(by_label.values(), by_label.keys(), loc='best', fontsize=9)\n",
    "\n",
    "\n",
    "def plot_comparison(\n",
    "    ax1, train_df, oot_df, bin_col_name, metric_column, \n",
    "    y_axis_label, train_totals_row, total_event_rate_oot, metric,\n",
    "    show_bar_values=False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Helper function to plot a comparison between training and OOT datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax1 : matplotlib axis\n",
    "        The primary axis for plotting\n",
    "    train_df : pandas DataFrame\n",
    "        Training data binning table\n",
    "    oot_df : pandas DataFrame\n",
    "        OOT data binning table\n",
    "    bin_col_name : str\n",
    "        Name of the bin column\n",
    "    metric_column : str\n",
    "        Name of the metric column ('Event rate' or 'WoE')\n",
    "    y_axis_label : str\n",
    "        Label for the y-axis\n",
    "    train_totals_row : pandas Series\n",
    "        Totals row from training data\n",
    "    total_event_rate_oot : float\n",
    "        Overall event rate for OOT data\n",
    "    metric : str\n",
    "        Metric to plot ('event_rate' or 'woe')\n",
    "    show_bar_values : bool\n",
    "        Whether to show Count (%) values on top of the bars\n",
    "    \"\"\"\n",
    "    # Ensure bin columns are string type for consistency\n",
    "    train_df[bin_col_name] = train_df[bin_col_name].astype(str)\n",
    "    oot_df[bin_col_name] = oot_df[bin_col_name].astype(str)\n",
    "    \n",
    "    # Create a combined set of unique bin names, preserving order from train_df first\n",
    "    bin_names = list(train_df[bin_col_name])\n",
    "    # Add any bins from OOT that aren't in training (shouldn't happen normally)\n",
    "    for bin_name in oot_df[bin_col_name]:\n",
    "        if bin_name not in bin_names:\n",
    "            bin_names.append(bin_name)\n",
    "    \n",
    "    # Create x indices\n",
    "    x_indices = np.arange(len(bin_names))\n",
    "    \n",
    "    # Prepare data for plotting - match bin names between datasets\n",
    "    train_values = []\n",
    "    oot_values = []\n",
    "    train_counts = []\n",
    "    oot_counts = []\n",
    "    \n",
    "    for bin_name in bin_names:\n",
    "        # Find metric values for each bin in each dataset\n",
    "        train_row = train_df[train_df[bin_col_name] == bin_name]\n",
    "        oot_row = oot_df[oot_df[bin_col_name] == bin_name]\n",
    "        \n",
    "        # If bin exists in training data, get its value; otherwise NaN\n",
    "        if not train_row.empty:\n",
    "            train_values.append(train_row[metric_column].values[0])\n",
    "            train_counts.append(train_row['Count (%)'].values[0] * 100)  # Convert to percentage\n",
    "        else:\n",
    "            train_values.append(np.nan)\n",
    "            train_counts.append(0)\n",
    "        \n",
    "        # If bin exists in OOT data, get its value; otherwise NaN\n",
    "        if not oot_row.empty:\n",
    "            oot_values.append(oot_row[metric_column].values[0])\n",
    "            oot_counts.append(oot_row['Count (%)'].values[0] * 100)  # Convert to percentage\n",
    "        else:\n",
    "            oot_values.append(np.nan)\n",
    "            oot_counts.append(0)\n",
    "    \n",
    "    # Format bin labels\n",
    "    bin_labels = []\n",
    "    for bin_name in bin_names:\n",
    "        if 'Special' in bin_name or 'Missing' in bin_name:\n",
    "            bin_labels.append(bin_name)\n",
    "        else:\n",
    "            shortened_name = str(bin_name)\n",
    "            if len(shortened_name) > 15:\n",
    "                shortened_name = shortened_name[:12] + \"...\"\n",
    "            bin_labels.append(shortened_name)\n",
    "    \n",
    "    # Plot Count (%) as grouped bars\n",
    "    bar_width = 0.35\n",
    "    bar1 = ax1.bar(x_indices - bar_width/2, train_counts, bar_width, color='#0a3f7d', alpha=0.7, label='Train Count (%)')\n",
    "    bar2 = ax1.bar(x_indices + bar_width/2, oot_counts, bar_width, color='#d35400', alpha=0.7, label='OOT Count (%)')\n",
    "    \n",
    "    # Add value labels on top of bars if requested\n",
    "    if show_bar_values:\n",
    "        for i, (b1, b2) in enumerate(zip(bar1, bar2)):\n",
    "            # Training bar values\n",
    "            h1 = b1.get_height()\n",
    "            if h1 > 0:\n",
    "                ax1.text(\n",
    "                    b1.get_x() + b1.get_width()/2.,\n",
    "                    h1 + 0.5,\n",
    "                    f'{h1:.1f}%',\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=8, color='navy'\n",
    "                )\n",
    "            \n",
    "            # OOT bar values\n",
    "            h2 = b2.get_height()\n",
    "            if h2 > 0:\n",
    "                ax1.text(\n",
    "                    b2.get_x() + b2.get_width()/2.,\n",
    "                    h2 + 0.5,\n",
    "                    f'{h2:.1f}%',\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=8, color='darkred'\n",
    "                )\n",
    "    \n",
    "    ax1.set_xticks(x_indices)\n",
    "    ax1.set_xticklabels(bin_labels, rotation=45, ha='right', fontsize=9)\n",
    "    ax1.set_xlabel('Bins', fontsize=10)\n",
    "    ax1.set_ylabel('Count (%)', color='blue', fontsize=10)\n",
    "    ax1.tick_params(axis='y', labelcolor='blue', labelsize=9)\n",
    "    ax1.legend(loc='upper left', fontsize=8)\n",
    "    \n",
    "    # Create second y-axis for Event Rate or WoE\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Identify regular, special, and missing bins\n",
    "    regular_mask = ~np.array([('Special' in b or 'Missing' in b) for b in bin_names])\n",
    "    regular_indices = np.where(regular_mask)[0]\n",
    "    special_indices = [i for i, b in enumerate(bin_names) if 'Special' in b]\n",
    "    missing_indices = [i for i, b in enumerate(bin_names) if 'Missing' in b]\n",
    "    \n",
    "    # Plot lines for regular bins\n",
    "    if len(regular_indices) > 0:\n",
    "        # Filter out NaN values for line plots\n",
    "        valid_train_idx = [i for i in regular_indices if not np.isnan(train_values[i])]\n",
    "        valid_oot_idx = [i for i in regular_indices if not np.isnan(oot_values[i])]\n",
    "        \n",
    "        if valid_train_idx:\n",
    "            # Plot training data line\n",
    "            train_line = ax2.plot([i for i in valid_train_idx], \n",
    "                                  [train_values[i] for i in valid_train_idx], \n",
    "                                  marker='o', color='darkgoldenrod', linewidth=2, \n",
    "                                  label=f'Train {y_axis_label}')\n",
    "            \n",
    "            # Add value annotations for training line\n",
    "            for idx in valid_train_idx:\n",
    "                ax2.annotate(f'{train_values[idx]:.3f}', \n",
    "                             xy=(idx, train_values[idx]), \n",
    "                             xytext=(0, 5),\n",
    "                             textcoords='offset points',\n",
    "                             ha='center', \n",
    "                             fontsize=7,\n",
    "                             color='darkgoldenrod')\n",
    "        \n",
    "        if valid_oot_idx:\n",
    "            # Plot OOT data line\n",
    "            oot_line = ax2.plot([i for i in valid_oot_idx], \n",
    "                                [oot_values[i] for i in valid_oot_idx], \n",
    "                                marker='s', color='darkgreen', linewidth=2, \n",
    "                                label=f'OOT {y_axis_label}')\n",
    "            \n",
    "            # Add value annotations for OOT line\n",
    "            for idx in valid_oot_idx:\n",
    "                ax2.annotate(f'{oot_values[idx]:.3f}', \n",
    "                             xy=(idx, oot_values[idx]), \n",
    "                             xytext=(0, -15),\n",
    "                             textcoords='offset points',\n",
    "                             ha='center', \n",
    "                             fontsize=7,\n",
    "                             color='darkgreen')\n",
    "    \n",
    "    # Plot Special bin points\n",
    "    for idx in special_indices:\n",
    "        if not np.isnan(train_values[idx]):\n",
    "            ax2.plot(idx, train_values[idx],\n",
    "                     marker='s', color='red', markersize=8, linestyle='None', \n",
    "                     label='Train Special' if idx == special_indices[0] else \"\")\n",
    "            ax2.annotate(f'{train_values[idx]:.3f}', \n",
    "                         xy=(idx, train_values[idx]), \n",
    "                         xytext=(-10, 5),\n",
    "                         textcoords='offset points',\n",
    "                         ha='center', \n",
    "                         fontsize=7,\n",
    "                         color='red')\n",
    "        \n",
    "        if not np.isnan(oot_values[idx]):\n",
    "            ax2.plot(idx, oot_values[idx],\n",
    "                     marker='s', color='darkred', markersize=8, linestyle='None', \n",
    "                     label='OOT Special' if idx == special_indices[0] else \"\")\n",
    "            ax2.annotate(f'{oot_values[idx]:.3f}', \n",
    "                         xy=(idx, oot_values[idx]), \n",
    "                         xytext=(10, 5),\n",
    "                         textcoords='offset points',\n",
    "                         ha='center', \n",
    "                         fontsize=7,\n",
    "                         color='darkred')\n",
    "    \n",
    "    # Plot Missing bin points\n",
    "    for idx in missing_indices:\n",
    "        if not np.isnan(train_values[idx]):\n",
    "            ax2.plot(idx, train_values[idx],\n",
    "                     marker='D', color='purple', markersize=8, linestyle='None', \n",
    "                     label='Train Missing' if idx == missing_indices[0] else \"\")\n",
    "            ax2.annotate(f'{train_values[idx]:.3f}', \n",
    "                         xy=(idx, train_values[idx]), \n",
    "                         xytext=(-10, 5),\n",
    "                         textcoords='offset points',\n",
    "                         ha='center', \n",
    "                         fontsize=7,\n",
    "                         color='purple')\n",
    "        \n",
    "        if not np.isnan(oot_values[idx]):\n",
    "            ax2.plot(idx, oot_values[idx],\n",
    "                     marker='D', color='darkmagenta', markersize=8, linestyle='None', \n",
    "                     label='OOT Missing' if idx == missing_indices[0] else \"\")\n",
    "            ax2.annotate(f'{oot_values[idx]:.3f}', \n",
    "                         xy=(idx, oot_values[idx]), \n",
    "                         xytext=(10, 5),\n",
    "                         textcoords='offset points',\n",
    "                         ha='center', \n",
    "                         fontsize=7,\n",
    "                         color='darkmagenta')\n",
    "    \n",
    "    # Add horizontal lines for totals\n",
    "    if train_totals_row is not None and metric_column in train_totals_row:\n",
    "        try:\n",
    "            train_total_metric = float(train_totals_row[metric_column]) if train_totals_row[metric_column] != '' else 0.0\n",
    "            ax2.axhline(y=train_total_metric, color='darkgoldenrod', linestyle='--', \n",
    "                       alpha=0.7, label=f'Train Total: {train_total_metric:.4f}')\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    if total_event_rate_oot is not None and metric == 'event_rate':\n",
    "        try:\n",
    "            oot_total_metric = float(total_event_rate_oot)\n",
    "            ax2.axhline(y=oot_total_metric, color='darkgreen', linestyle='--', \n",
    "                       alpha=0.7, label=f'OOT Total: {oot_total_metric:.4f}')\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    # Add IV information to legend if available for training data\n",
    "    if train_totals_row is not None and 'IV' in train_totals_row and train_totals_row['IV'] not in (None, '', np.nan):\n",
    "        try:\n",
    "            iv_value_train = float(train_totals_row['IV'])\n",
    "            ax2.plot([], [], ' ', label=f'Train IV: {iv_value_train:.4f}')\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    # Set y-axis label and legend\n",
    "    ax2.set_ylabel(y_axis_label, color='black', fontsize=11)\n",
    "    ax2.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Add legend with unique entries\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax2.legend(by_label.values(), by_label.keys(), loc='upper right', fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing training data with a newer time period\n",
    "unified_bivariate_analysis(\n",
    "    binning_process=binning_process,\n",
    "    filename='train_validation_new7',\n",
    "    metric='event_rate',\n",
    "    oot_data=test_data,  # Data from a more recent time period\n",
    "    target_column=target,\n",
    "    compare_data=True ,\n",
    "    show_bar_values=True # Show both datasets side by side\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of final selected variables\n",
    "final_predictors=list(binning_process.get_support(names=True))\n",
    "#final_predictors = ['loyalty_score','credit_score','customer_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into woe or binned values or event rate\n",
    "X_train_WOE = binning_process.transform(X_train,metric='woe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example to understand the how binning object trasform the data to woe values\n",
    "test_exmaple = pd.DataFrame([{'age': -999,\n",
    " 'income': -0.8276048978295478,\n",
    " 'experience_years': -0.7913895315832409,\n",
    " 'credit_score': -0.7389923463082454,\n",
    " 'avg_spend': 0.7331633070112105,\n",
    " 'loyalty_score': -0.8702525202009143,\n",
    " 'satisfaction_rating': 2.237864030340072,\n",
    " 'demographic_index': -0.3085181724416579,\n",
    " 'financial_status': -0.0191376466062278,\n",
    " 'work_experience': -1.812865565832083,\n",
    " 'credit_index': 0.5505331954414358,\n",
    " 'customer_group': 'group_B'}])\n",
    "\n",
    "test_WOE = binning_process.transform(test_exmaple,metric='woe')\n",
    "test_WOE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
